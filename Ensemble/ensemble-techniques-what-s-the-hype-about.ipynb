{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T13:08:55.104735Z","iopub.execute_input":"2022-04-04T13:08:55.105119Z","iopub.status.idle":"2022-04-04T13:08:55.139779Z","shell.execute_reply.started":"2022-04-04T13:08:55.105016Z","shell.execute_reply":"2022-04-04T13:08:55.139125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://repository-images.githubusercontent.com/206526243/c6c83680-d003-11e9-8eda-4296871ca94a\">","metadata":{}},{"cell_type":"markdown","source":"## This Notebook is going to be a deep dive into Ensemble techniques ü¶à\n\n#### Ensemble techniques leverage the strengths of Multiple Machine Learning models and improve the overall performacne.\n\nLately ensemble techniques have been heavily used in the Notebooks of Kaggle Competition winners üèÖü•áü•à and are gaining massive popularity.In fact owing to their massive gain in popularity they are being extensively used in Hackathons and Kaggle competitions. ","metadata":{}},{"cell_type":"markdown","source":"# What is going to be covered\n\n<b>This is an exhaustive list of what is going to be covered in this Notebook. The Notebook will cover the theory, intuition as well as the Implementation of all of the following points in great depth. This notebook can be used as your cheatsheet to Ensemble techniques </b>\n\n### Simple Ensemble Techniques\n1. Max Voting (Hard Voting)\n2. Averaging (Soft Voting)\n\n## Slightly advanced concepts üéØ\n\n### Bagging üí∞\n\n1. Bagging Meta Estimator\n2. Random Forrest \n\n\n### Boosting  üöÑ\n\n1. Adaptive Boosting AdaBoost\n2. CatBoost\n3. XGBoost\n4. GBM\n5. Light GBM\n6. XGBM (Xtreme GBM) <br>GBM * -> Gradient Boosting Machines\n\n### Stacking üåà\n\n1. Stacking widely used Models like Decision trees, KNN, SVM, SGDs, Logistic Regressors\n\n### Blending \n\n1. Applying Blending techniques to the stacked Models by breaking down the data into a validation set.","metadata":{}},{"cell_type":"markdown","source":"##  How are ensemble techniques this good !!! üí™\n\n<b>A mean squared error can be decomposed into Bias and Variance. One of the main objectives of building an ensemble is to achieve a Final Model that has a low bias and a low variance. This can be done by combining individual models to derive low variance model and still maintain a pretty low bias. For these we use Individual discrete models and name them as base learners. Now how we create the base learners is upto us and there are multiple methods to go around this. \n</b>\n1. We can tune the hyperparameters for the same model to create different base learners\n2. We can choose different models like SVM, Decision Trees, KNN, Logistics Classifiers to be individual base learners.\n3. Different data subsets sent to the same model\n4. Giving different feature sets to different base learners.\n    \n#### Each of the base learner is independent from each other and so intuitively and mathematically it can be proven that ensembles do better than standalone models.\n\n<b>\nSuppose we have 3 Base Learners all with an accuracy of 80%.\nAll of them are merged to make an Ensemble then let's see how our accuracy is boosted(Here we would assume that the final Ensemble would take 2 correct ensembles as the benchmark for identifying the datapoint correctly).\n    </b>\n\nPr--> Probability\nPr(All 3 BL are accurate) = 0.8* 0.8* 0.8 <br>\nPr(Any two of them are correct) = 0.8 * 0.8 * 0.2 <br>\nPr(Any two of them are correct) = 0.8 * 0.2 * 0.8 <br>\nPr(Any two of them are correct) = 0.2 * 0.8 * 0.8 <br>\nAdding them mto get the Pr that the Ensemble is correct: <br>\n<b> 0.512 + 0.384  = 0.896  </b> \nAnd this is the lower end of accuracy and this can further be boosted.   üòéüòéüòé  \nSo this will definitely boost the accuracy from all the 3 Base Learners and as we scale asnd increase the number of Base Learners and move on to more sophisticated techniques we can see that this number can be increases even further.","metadata":{}},{"cell_type":"markdown","source":"## Voting üíÇ‚Äç\n#### Voting happens between the Models and the majority result is given the impetus and so the prediction has to be the mode of the predictions received from each model.\n#### Basically each model casts a vote on what the Prediction should be and then the class that gets the majority wins and is taken as the Final Prediction.\n\n\n**What do we need to ensure:**\nThe Models need to be different and varied. They represent the diversity that is needed so that we are able to get votes from a plethora of algorithms and then probably take a richer decision. Then once we have the votes from individual models we can derive a final prediction","metadata":{}},{"cell_type":"code","source":"# Importing some Important library dependencies and Datasets\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:55.143920Z","iopub.execute_input":"2022-04-04T13:08:55.144300Z","iopub.status.idle":"2022-04-04T13:08:56.557286Z","shell.execute_reply.started":"2022-04-04T13:08:55.144260Z","shell.execute_reply":"2022-04-04T13:08:56.556575Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Here we will use 4 Models as Base Learners -> KNN, Decision Trees, SVM and Logistic Regressor\ndata = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.558736Z","iopub.execute_input":"2022-04-04T13:08:56.559874Z","iopub.status.idle":"2022-04-04T13:08:56.616749Z","shell.execute_reply.started":"2022-04-04T13:08:56.559828Z","shell.execute_reply":"2022-04-04T13:08:56.615794Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# So the Only Null Values are in BMI, and since this is a small fraction of 5110, all of these\n# Columns can be removes\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.617906Z","iopub.execute_input":"2022-04-04T13:08:56.618141Z","iopub.status.idle":"2022-04-04T13:08:56.630242Z","shell.execute_reply.started":"2022-04-04T13:08:56.618114Z","shell.execute_reply":"2022-04-04T13:08:56.629499Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# The cleaning step will Only Involve the dropping bmi columns with null values and Dropping other genders due to low presence in data\n# Also the Id column is dropped since it has no role to play here\ndata = data.dropna(subset=['bmi'])\ndata = data[data['gender'] != 'Other']\ndata = data.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.631825Z","iopub.execute_input":"2022-04-04T13:08:56.632447Z","iopub.status.idle":"2022-04-04T13:08:56.655831Z","shell.execute_reply.started":"2022-04-04T13:08:56.632412Z","shell.execute_reply":"2022-04-04T13:08:56.655129Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.657164Z","iopub.execute_input":"2022-04-04T13:08:56.658184Z","iopub.status.idle":"2022-04-04T13:08:56.676749Z","shell.execute_reply.started":"2022-04-04T13:08:56.658112Z","shell.execute_reply":"2022-04-04T13:08:56.675864Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Encoding the Categorical Features, object type to Labels\ncat_feat = data.dtypes[data.dtypes == 'O'].index.values\nle = LabelEncoder()\n\nfor i in cat_feat:\n    data[i] = le.fit_transform(data[i])\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.678239Z","iopub.execute_input":"2022-04-04T13:08:56.678579Z","iopub.status.idle":"2022-04-04T13:08:56.713928Z","shell.execute_reply.started":"2022-04-04T13:08:56.678547Z","shell.execute_reply":"2022-04-04T13:08:56.713076Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Defining the data and creating the Training and Test Set\nX = data.drop('stroke', axis=1)\ny = data['stroke']\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.715234Z","iopub.execute_input":"2022-04-04T13:08:56.716132Z","iopub.status.idle":"2022-04-04T13:08:56.730747Z","shell.execute_reply.started":"2022-04-04T13:08:56.716097Z","shell.execute_reply":"2022-04-04T13:08:56.730076Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Handling the Imbalanced Data  to generate more synthetic examples of the Minority class via SMOTE\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nX_bal, Y_bal = smote.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_bal, Y_bal, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.731758Z","iopub.execute_input":"2022-04-04T13:08:56.735196Z","iopub.status.idle":"2022-04-04T13:08:56.892848Z","shell.execute_reply.started":"2022-04-04T13:08:56.735128Z","shell.execute_reply":"2022-04-04T13:08:56.891800Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Here we will use some Standalone mmodels LR, KNN, Decision Trees, Naive Bayes and SVM Classifier\nall_model = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(),\n            BernoulliNB(), SVC()]\n\nrecall = []\nprecision = []\nf1=[]\nbalanced_accuracy=[]\naccuracy=[]\n\nfor model in all_model:\n    \n    \n    cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=10).mean()\n    recall.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=10).mean()\n    precision.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=10).mean()\n    f1.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='balanced_accuracy', cv=10).mean()\n    balanced_accuracy.append(cv)\n\nmodel = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier',\n        'BernoulliNB', 'SVC']\n\nscore = pd.DataFrame({'Model': model, 'Precision': precision, 'Recall': recall, 'F1':f1, 'balanced_accuracy':balanced_accuracy})\nscore.style.background_gradient(high=1,axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:08:56.894051Z","iopub.execute_input":"2022-04-04T13:08:56.894270Z","iopub.status.idle":"2022-04-04T13:10:00.694719Z","shell.execute_reply.started":"2022-04-04T13:08:56.894243Z","shell.execute_reply":"2022-04-04T13:10:00.693886Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Hard vs Soft Voting:\n\nIn classification problems, there are two types of voting: hard voting and soft voting. Hard voting entails picking the prediction with the highest number of votes, whereas soft voting entails combining the probabilities of each prediction in each model and picking the prediction with the highest total probability.\n\n\n1. Hard voting is a simple Majority Voting scheme\n2. In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n","metadata":{}},{"cell_type":"code","source":"# Now we will use a Voting Technique to see how we are able to boost out results\n\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(random_state=42)\nclf2 = RandomForestClassifier(random_state=42)\nclf3 = GaussianNB()\nclf4 = SVC(probability=True, random_state=42)\nclf5 = KNeighborsClassifier(5)\nclf6 = DecisionTreeClassifier()\neclf = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GNB', clf3), ('SVC', clf4),(\"KNN\",clf5),(\"DT\",clf6)],\n                        voting='hard', weights=[1,2,1,1,1,2])\neclf.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:10:00.697123Z","iopub.execute_input":"2022-04-04T13:10:00.697359Z","iopub.status.idle":"2022-04-04T13:10:09.891138Z","shell.execute_reply.started":"2022-04-04T13:10:00.697332Z","shell.execute_reply":"2022-04-04T13:10:09.890171Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_pred=eclf.predict(X_test)\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:10:09.892451Z","iopub.execute_input":"2022-04-04T13:10:09.892683Z","iopub.status.idle":"2022-04-04T13:10:10.520426Z","shell.execute_reply.started":"2022-04-04T13:10:09.892654Z","shell.execute_reply":"2022-04-04T13:10:10.519330Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Trying the same with Soft voting/ Averaging technique\n# Creating Individual Classifiers again:\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(random_state=42)\nclf2 = RandomForestClassifier(random_state=42)\nclf3 = GaussianNB()\nclf4 = SVC(probability=True, random_state=42)\nclf5 = KNeighborsClassifier(5)\nclf6 = DecisionTreeClassifier()\neclf = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GNB', clf3), ('SVC', clf4),(\"KNN\",clf5),(\"DT\",clf6)],\n                        voting='soft', weights=[1,2,1,1,1,2])\neclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:10:10.521717Z","iopub.execute_input":"2022-04-04T13:10:10.521934Z","iopub.status.idle":"2022-04-04T13:10:19.934785Z","shell.execute_reply.started":"2022-04-04T13:10:10.521909Z","shell.execute_reply":"2022-04-04T13:10:19.933981Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y_pred=eclf.predict(X_test)\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:10:19.936214Z","iopub.execute_input":"2022-04-04T13:10:19.937119Z","iopub.status.idle":"2022-04-04T13:10:20.481377Z","shell.execute_reply.started":"2022-04-04T13:10:19.937070Z","shell.execute_reply":"2022-04-04T13:10:20.480514Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Bagging Techniques\n\nAs we saw the Simple Ensemble Techniques are quite intuitive and can be very easily applied in this case. Now we move to more sophisticated techniques that are not as trivial as the Previous One. We will first be studying Bagging Techniques. These are the bagging techniques.\n\n1. Random Forest Algorithms  (Bagging Decision Tree)\n2. Bagging Meta Estimator\n3. Bagging Individual Algorithms like SVM, KNN to see how they work\n","metadata":{}},{"cell_type":"markdown","source":"## Theory - Bagging\n#### Also called as Bootstrapped aggregator\n\n**So what  do we do in Bagging, really ü§î**\n\n1. We create n different Base Learning Models\n2. We give each Model a subset of the Dataset available (The subset is not necessarily unique and we employ a Row sampling technique with replacements allowed, so some records may be duplicated across subsets of data being fed to different models) Ps- We can also do the sampling on the basis of the features for a better sampling.\n3. The training data will get simultaneously trained on the Models (Parallel)\n4. Then we pass the train data to each Model and get the Predictions\n5. The next step invloves using a voting classifier to get the majority vote for which prediction should we go ahead with\nSo each Base Learner is like a Bag with some samples from the datasets.\n\n### Random Forest üå≥üå≤üå¥\n* <img src=\"https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG\" heigth=630 width=630>\n\n\nHere in a Random Forest Classifier each Base Learner turns out to be a Decision Tree. So all of our Decision trees are fed different subsets and samples from the Data.  As each decision tree has a low bias and High Variance. Low bias means it is not very erroneous if the it is trained on the training data for the complete depth. High variance occurs when we introduce new set of data and so they are prone to create errors.\n\n#### So here is why aggregation step becomes important\n\nWe have a set of n-Predictions for n Base Learners a.k.a Decision Trees in this case \nEach of them have a Low Bias and High Variance on the Test Data\nWhen we aggregate all of them with Majority Voting the High Variance gets converted to Low Variance\n\n### A quick difference between Random Forest and a Decision Tree Bagging Ensemble\n<img src=\"https://i.stack.imgur.com/sYR7y.png\">\n\n**Some Maths to Prove this üòÉ**\n\nSuppose we have an RV -> X\nIt is a normally distributed Radnom Variable with (u(Mean), S^2 (Variance)\nIf we sample the RV once the mean and the Variance are going to remain the same\nIf we sample it n Times -> and we generate a new RV constructed from the average of the samples of each Base Leaners here.\nX' = (x1+x2+x3+x4+x5...xn)/n then\n1. The mean will still remain to be u since N*(u/N)\n2. The Variance will change to S^2/N because N*(var(x1)+var(x2)...var(xn)/N^2)\nSo this becomes a Low Bias and a \"Low Variance\" Model in this way.","metadata":{}},{"cell_type":"markdown","source":"## What happens if this turns out to be a regression Problem\nSo if that  turns out to be the case then we might not have the prediction output as a class but rather a continous numerical value. In that case we might have to take the mean of all the values present and then the mean will become our final Prediction.","metadata":{}},{"cell_type":"markdown","source":"### Bagging with Decision Trees as Base Learners","metadata":{}},{"cell_type":"code","source":"# test classification dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T13:10:20.482623Z","iopub.execute_input":"2022-04-04T13:10:20.483184Z","iopub.status.idle":"2022-04-04T13:10:24.566574Z","shell.execute_reply.started":"2022-04-04T13:10:20.483150Z","shell.execute_reply":"2022-04-04T13:10:24.565766Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classifiers","metadata":{}},{"cell_type":"code","source":"","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A Support Vector Machine Bagging Example","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A KNN Bagging Ensemble with Selective Features","metadata":{}},{"cell_type":"markdown","source":"### Boosting Techniques\n\nBoosting techniques create sequential Models as Base Learners and then data subsets are passed through them to make the overall performance robust.This will be the general workflow.\n1. Create a base Learner\n2. Pass a subset of data onto the Base Learner \n3. Now pass the full data to evaluate the wrong Predictions by the Model.\n4. Now create a base learner 2 and only pass on the data points that were incorrectly predicted by the Base Learner 1 as training data to the Base Learner 2.\n5. Now again pass the Entire dataset on Base Learner 2 to evaluate the wrong predictions.\n6. This base learner will probably classify some other datapoints incorrectly.\n7. These datapoints specifically will be passed on as training data to Base Learner 3.\n8. This is a sequential process and this can go until we specify the number of BLs or steps.\n9. Here is a good image to understand Bagging in a pictorial way.\n<img src=\"https://pluralsight2.imgix.net/guides/a9a5ff4e-b617-4afe-b27b-d96793defa87_6.jpg\">\n","metadata":{}}]}
