{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the Project that I've been working on while taking this [Course](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/learn/)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-14T11:26:38.749863Z","iopub.execute_input":"2022-01-14T11:26:38.750261Z","iopub.status.idle":"2022-01-14T11:26:38.825133Z","shell.execute_reply.started":"2022-01-14T11:26:38.750165Z","shell.execute_reply":"2022-01-14T11:26:38.823166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:38.826907Z","iopub.execute_input":"2022-01-14T11:26:38.82732Z","iopub.status.idle":"2022-01-14T11:26:44.982499Z","shell.execute_reply.started":"2022-01-14T11:26:38.827284Z","shell.execute_reply":"2022-01-14T11:26:44.981579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir='../input/200000-abstracts-for-seq-sentence-classification/20k_abstracts_numbers_with_@/'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:44.983461Z","iopub.execute_input":"2022-01-14T11:26:44.98368Z","iopub.status.idle":"2022-01-14T11:26:44.990508Z","shell.execute_reply.started":"2022-01-14T11:26:44.983653Z","shell.execute_reply":"2022-01-14T11:26:44.989587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames=[data_dir + filename for filename in os.listdir(data_dir)]\nfilenames","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:44.992233Z","iopub.execute_input":"2022-01-14T11:26:44.992769Z","iopub.status.idle":"2022-01-14T11:26:45.015362Z","shell.execute_reply.started":"2022-01-14T11:26:44.992719Z","shell.execute_reply":"2022-01-14T11:26:45.014468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Preprocessing the Data","metadata":{}},{"cell_type":"code","source":"# Creating a function to read the txt Files\n# This function returns all the lines in the txt file as a list\ndef get_lines(filename):\n    with open(filename,\"r\") as f:\n        return f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:45.019551Z","iopub.execute_input":"2022-01-14T11:26:45.019924Z","iopub.status.idle":"2022-01-14T11:26:45.02519Z","shell.execute_reply.started":"2022-01-14T11:26:45.01988Z","shell.execute_reply":"2022-01-14T11:26:45.023982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the Lines in the Training Set\ntrain_data_lines=get_lines(data_dir+\"train.txt\")\n# train_data_lines[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:45.027119Z","iopub.execute_input":"2022-01-14T11:26:45.027445Z","iopub.status.idle":"2022-01-14T11:26:45.66006Z","shell.execute_reply.started":"2022-01-14T11:26:45.027403Z","shell.execute_reply":"2022-01-14T11:26:45.659146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <h4>\n Pre-processing the Data <br>\n List of dictionaries <br>\n Each line needs to be converted into a dictionary <br>\n Dictionary items are ordered, changeable, and does not allow duplicates.<br>\n Objectives, Methods, Results, Conclusins will all be Values to the 'TARGET' KEY<br>\n And the Text corresponding to them will be Values to the Key 'TEXT'<br>\n So each abstract would have about roughly 10-12 dicts for each statement<br>","metadata":{}},{"cell_type":"code","source":"# Preprocessing Functions\n# Returns a list of dictionaries of abstract's lines\n# Dict Format --> {'TARGET':'Background/Results/Objetive/Concludion','Text':'The actual statement'}\ndef preprocess_data(filename):\n    input_lines=get_lines(filename)\n    #This will be used to separte the abstracts from  one another using String mets\n    abstract_lines=\"\"\n    # Empty list of abstracts\n    abstract_samples=[]\n    for line in input_lines:\n        # Check for a new abstract\n        if line.startswith(\"###\"):\n            abstract_id=line\n            # And since we are in a new abstract we will Reset the abstract_lines\n            abstract_lines=\"\"\n        # Check for a new line \\n escape seq\n        elif line.isspace():\n            # Split the Lines of the abstract and will return a list of one abstract\n            abstract_line_split=abstract_lines.splitlines()\n            # Now we have to iterate through this singular abstract\n            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n                #  Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object.\n                # Create a empty Dict per line\n                line_data={}\n                # Split on the tab \\t esc seq\n                target_text_split=abstract_line.split(\"\\t\")\n                # Get the Label of the sentence as the Label\n                line_data[\"target\"]=target_text_split[0]\n                # Get the Text of the Lien as the Text Key\n                line_data[\"text\"]=target_text_split[1].lower()\n                # Also adding the Line Nnumber as it will also aid the model\n                line_data[\"line_number\"]=abstract_line_number\n                # Number of Lines in that particular abstract\n                line_data[\"total_lines\"]=len(abstract_line_split)-1\n                # Now we have to append them to the absract_samples list\n                abstract_samples.append(line_data)\n        # So if both the cases are not there then the line is a labelled sentence\n        else:\n            abstract_lines+=line\n    return abstract_samples\n                \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:45.661376Z","iopub.execute_input":"2022-01-14T11:26:45.661606Z","iopub.status.idle":"2022-01-14T11:26:45.677572Z","shell.execute_reply.started":"2022-01-14T11:26:45.661579Z","shell.execute_reply":"2022-01-14T11:26:45.676935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Get the data and preprocess it\ntrain_samples=preprocess_data(data_dir+\"train.txt\")\nval_samples=preprocess_data(data_dir+\"dev.txt\")\ntest_samples=preprocess_data(data_dir+\"test.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:45.678492Z","iopub.execute_input":"2022-01-14T11:26:45.679178Z","iopub.status.idle":"2022-01-14T11:26:46.703827Z","shell.execute_reply.started":"2022-01-14T11:26:45.679133Z","shell.execute_reply":"2022-01-14T11:26:46.70253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_samples),len(val_samples),len(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:46.705214Z","iopub.execute_input":"2022-01-14T11:26:46.705446Z","iopub.status.idle":"2022-01-14T11:26:46.713659Z","shell.execute_reply.started":"2022-01-14T11:26:46.705419Z","shell.execute_reply":"2022-01-14T11:26:46.712428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the Data\ntrain_samples[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:46.715722Z","iopub.execute_input":"2022-01-14T11:26:46.716098Z","iopub.status.idle":"2022-01-14T11:26:46.731196Z","shell.execute_reply.started":"2022-01-14T11:26:46.71605Z","shell.execute_reply":"2022-01-14T11:26:46.730595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we have to turn this data into a df\ntrain_df=pd.DataFrame(train_samples)\ntest_df=pd.DataFrame(test_samples)\nval_df=pd.DataFrame(val_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:46.732119Z","iopub.execute_input":"2022-01-14T11:26:46.732327Z","iopub.status.idle":"2022-01-14T11:26:47.160228Z","shell.execute_reply.started":"2022-01-14T11:26:46.732301Z","shell.execute_reply":"2022-01-14T11:26:47.158894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(11)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.161798Z","iopub.execute_input":"2022-01-14T11:26:47.162213Z","iopub.status.idle":"2022-01-14T11:26:47.183776Z","shell.execute_reply.started":"2022-01-14T11:26:47.162163Z","shell.execute_reply":"2022-01-14T11:26:47.182699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Spread of Data\ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.185337Z","iopub.execute_input":"2022-01-14T11:26:47.185765Z","iopub.status.idle":"2022-01-14T11:26:47.235784Z","shell.execute_reply.started":"2022-01-14T11:26:47.185719Z","shell.execute_reply":"2022-01-14T11:26:47.234557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Length of Lines\ntrain_df.total_lines.plot.hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.240495Z","iopub.execute_input":"2022-01-14T11:26:47.241375Z","iopub.status.idle":"2022-01-14T11:26:47.564754Z","shell.execute_reply.started":"2022-01-14T11:26:47.241318Z","shell.execute_reply":"2022-01-14T11:26:47.563734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of just the text Columns\ntrain_sentences=train_df[\"text\"].tolist()\ntest_sentences=test_df[\"text\"].tolist()\nval_sentences=val_df[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.566223Z","iopub.execute_input":"2022-01-14T11:26:47.566496Z","iopub.status.idle":"2022-01-14T11:26:47.582026Z","shell.execute_reply.started":"2022-01-14T11:26:47.566466Z","shell.execute_reply":"2022-01-14T11:26:47.58071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_sentences),len(test_sentences),len(val_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.583503Z","iopub.execute_input":"2022-01-14T11:26:47.583838Z","iopub.status.idle":"2022-01-14T11:26:47.598057Z","shell.execute_reply.started":"2022-01-14T11:26:47.583773Z","shell.execute_reply":"2022-01-14T11:26:47.597388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning the target Labels into Numeric Data\n# We have 5 main labels -> Backgroun, Objective,Methods, Results, Conclusion\n# We'll encode them both 1HEC and Simple Nuemrical\nfrom sklearn.preprocessing import OneHotEncoder\n# Tensorflow is incompaible with sparse matrices\none_hot_encoder=OneHotEncoder(sparse=False)\n# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\ntrain_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\nval_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\ntest_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\ntrain_labels_one_hot,val_labels_one_hot,test_labels_one_hot","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.599371Z","iopub.execute_input":"2022-01-14T11:26:47.600034Z","iopub.status.idle":"2022-01-14T11:26:47.943897Z","shell.execute_reply.started":"2022-01-14T11:26:47.599994Z","shell.execute_reply":"2022-01-14T11:26:47.942593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding also for Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain_labels_encoded=le.fit_transform(train_df[\"target\"])\ntest_labels_encoded=le.fit_transform(test_df[\"target\"])\nval_labels_encoded=le.fit_transform(val_df[\"target\"])\ntrain_labels_encoded, test_labels_encoded,val_labels_encoded","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:47.945349Z","iopub.execute_input":"2022-01-14T11:26:47.945672Z","iopub.status.idle":"2022-01-14T11:26:48.056844Z","shell.execute_reply.started":"2022-01-14T11:26:47.945612Z","shell.execute_reply":"2022-01-14T11:26:48.055831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retieving classes \nnum_classes=len(le.classes_)\nclass_names=le.classes_\nnum_classes,class_names","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:48.058478Z","iopub.execute_input":"2022-01-14T11:26:48.058808Z","iopub.status.idle":"2022-01-14T11:26:48.06749Z","shell.execute_reply.started":"2022-01-14T11:26:48.058765Z","shell.execute_reply":"2022-01-14T11:26:48.066675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>  Model -> Naive Bayes Model </h3> <br>\n<h4> TF-IDF Multinomial Naive Bayes Classifier </h4>","metadata":{}},{"cell_type":"code","source":"# Turning them to tensors\n# Baseline Model\n# tfidf turns text into Numbers with the Formula\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n# Creating a Pipeline\n# A pipeline takes Multiple Tuples\nmodel_0=Pipeline([\n    (\"tf-idf\",TfidfVectorizer()),\n    (\"clf\",MultinomialNB())\n    \n])\nmodel_0.fit(train_sentences,train_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:48.069137Z","iopub.execute_input":"2022-01-14T11:26:48.069811Z","iopub.status.idle":"2022-01-14T11:26:53.729021Z","shell.execute_reply.started":"2022-01-14T11:26:48.069764Z","shell.execute_reply":"2022-01-14T11:26:53.728109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation and Prediction\n# In scikit learn it is score for eval\nmodel_0.score(val_sentences,val_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:53.730496Z","iopub.execute_input":"2022-01-14T11:26:53.730718Z","iopub.status.idle":"2022-01-14T11:26:54.680717Z","shell.execute_reply.started":"2022-01-14T11:26:53.730691Z","shell.execute_reply":"2022-01-14T11:26:54.679719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make Predictions for the Baseline Model\nbaseline_predictions=model_0.predict(val_sentences)\nbaseline_predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:54.682164Z","iopub.execute_input":"2022-01-14T11:26:54.682566Z","iopub.status.idle":"2022-01-14T11:26:55.589509Z","shell.execute_reply.started":"2022-01-14T11:26:54.682524Z","shell.execute_reply":"2022-01-14T11:26:55.588939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n# Models for Calculating different evaluation metrics\n# Returns a dict of different metrics\ndef calculate_results(y_true, y_pred):\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:55.590402Z","iopub.execute_input":"2022-01-14T11:26:55.591239Z","iopub.status.idle":"2022-01-14T11:26:55.597572Z","shell.execute_reply.started":"2022-01-14T11:26:55.591206Z","shell.execute_reply":"2022-01-14T11:26:55.596566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_results(val_labels_encoded,baseline_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:55.599515Z","iopub.execute_input":"2022-01-14T11:26:55.600128Z","iopub.status.idle":"2022-01-14T11:26:55.624244Z","shell.execute_reply.started":"2022-01-14T11:26:55.60008Z","shell.execute_reply":"2022-01-14T11:26:55.623423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Conv 1D Model </h3>","metadata":{}},{"cell_type":"code","source":"# Vectorize the text and then create Embeddings\nfrom tensorflow.keras import layers\n# How long is each sentence on average\nsent_lens=[len(sentence.split()) for sentence in train_sentences]\navg_sent_lens=np.mean(sent_lens)\navg_sent_lens\n# sent_lens\n#  So we will need Padding and Truncating as the input shapes must be maintained","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:55.626045Z","iopub.execute_input":"2022-01-14T11:26:55.626604Z","iopub.status.idle":"2022-01-14T11:26:57.081597Z","shell.execute_reply.started":"2022-01-14T11:26:55.62656Z","shell.execute_reply":"2022-01-14T11:26:57.080606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(sent_lens,bins=9)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:57.082804Z","iopub.execute_input":"2022-01-14T11:26:57.08301Z","iopub.status.idle":"2022-01-14T11:26:58.359393Z","shell.execute_reply.started":"2022-01-14T11:26:57.082985Z","shell.execute_reply":"2022-01-14T11:26:58.358713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentile of length of sentences\noutput_seq_len=int(np.percentile(sent_lens,95))\noutput_seq_len\n# So 95% sentences are in length of 55","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:58.360417Z","iopub.execute_input":"2022-01-14T11:26:58.360759Z","iopub.status.idle":"2022-01-14T11:26:58.408739Z","shell.execute_reply.started":"2022-01-14T11:26:58.360729Z","shell.execute_reply":"2022-01-14T11:26:58.407695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a text Vectorization Layer\n# Mapping our text from words to Numbers\n# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n# Vocabulary size in the Research Paper is 68000\nmax_tokens=68000\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:58.410337Z","iopub.execute_input":"2022-01-14T11:26:58.411186Z","iopub.status.idle":"2022-01-14T11:26:58.488409Z","shell.execute_reply.started":"2022-01-14T11:26:58.411143Z","shell.execute_reply":"2022-01-14T11:26:58.487756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapt the Text Vectorizer to the Training Data\n# We have to adapt it to only the training data so that val and test data are not seen\n# Later it can be fitted to the two latter\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:26:58.489581Z","iopub.execute_input":"2022-01-14T11:26:58.489951Z","iopub.status.idle":"2022-01-14T11:27:10.017563Z","shell.execute_reply.started":"2022-01-14T11:26:58.489921Z","shell.execute_reply":"2022-01-14T11:27:10.015991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding out how many words are there  in the training vocabulary and which are  most common\n# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\ntrain_vocab=text_vectorizer.get_vocabulary()\n# Size of Vocab\nprint(len(train_vocab))\n# 5 Most Common Words in the Vocab\nprint(train_vocab[:5])\n# Least common 5 words in the vocab\nprint(train_vocab[-5:])","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:10.020115Z","iopub.execute_input":"2022-01-14T11:27:10.020449Z","iopub.status.idle":"2022-01-14T11:27:10.410857Z","shell.execute_reply.started":"2022-01-14T11:27:10.020407Z","shell.execute_reply":"2022-01-14T11:27:10.408543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the config of our Text Vectorizer\ntext_vectorizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:10.414268Z","iopub.execute_input":"2022-01-14T11:27:10.4161Z","iopub.status.idle":"2022-01-14T11:27:10.434134Z","shell.execute_reply.started":"2022-01-14T11:27:10.415946Z","shell.execute_reply":"2022-01-14T11:27:10.430861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an Embedding Layer\n# More output dims , more emmbedding, more parameters to train\n# Masking the 0 considering themm as padding\ntoken_embed=layers.Embedding(input_dim=len(train_vocab),output_dim=128,mask_zero=True,name=\"token_embedding\")","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:10.436787Z","iopub.execute_input":"2022-01-14T11:27:10.43797Z","iopub.status.idle":"2022-01-14T11:27:10.45514Z","shell.execute_reply.started":"2022-01-14T11:27:10.437925Z","shell.execute_reply":"2022-01-14T11:27:10.452825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a Fast Loadinng Dataset with tf data API\n# https://www.tensorflow.org/guide/data_performance\n# https://www.tensorflow.org/guide/data\n# Turn our data into Tensorflow datasets\ntrain_dataset=tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\nval_dataset=tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\ntest_dataset=tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\ntrain_dataset\n# <TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>\n# Which indicates one Text Sample in first tuple, next tuple is (0,0,0,0,1) -> 1hc ","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:10.457098Z","iopub.execute_input":"2022-01-14T11:27:10.457691Z","iopub.status.idle":"2022-01-14T11:27:11.901001Z","shell.execute_reply.started":"2022-01-14T11:27:10.457613Z","shell.execute_reply":"2022-01-14T11:27:11.900063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre fetching the data and making them into batches\n# Pre fetching reduces the Preparation time of Data taken by CPU\n# Pref-fetching in a Multi-threaded way Reduces time and Increases the amount of data as all cores can be utilized to Prepare the Data\n# The GPU will do the Computation\ntrain_dataset=train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset=val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset=test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntrain_dataset\n# Run the Previous steps as well this otherwie the Shapes will not be fixed\n# PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:11.903097Z","iopub.execute_input":"2022-01-14T11:27:11.903427Z","iopub.status.idle":"2022-01-14T11:27:11.916243Z","shell.execute_reply.started":"2022-01-14T11:27:11.903377Z","shell.execute_reply":"2022-01-14T11:27:11.915317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\ninputs=layers.Input(shape=(1,),dtype=tf.string)\ntext_vectors=text_vectorizer(inputs)\ntoken_embedding=token_embed(text_vectors)\nx=layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embedding)\nx=layers.GlobalAveragePooling1D()(x)\noutputs=layers.Dense(num_classes,activation=\"softmax\")(x)\n# Indirect way of creating the Modelling the op ip\nmodel_1=tf.keras.Model(inputs,outputs)\n# Compiling the Model\nmodel_1.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:39.744712Z","iopub.execute_input":"2022-01-14T11:27:39.745025Z","iopub.status.idle":"2022-01-14T11:27:39.831342Z","shell.execute_reply.started":"2022-01-14T11:27:39.744984Z","shell.execute_reply":"2022-01-14T11:27:39.830304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:27:54.694615Z","iopub.execute_input":"2022-01-14T11:27:54.694972Z","iopub.status.idle":"2022-01-14T11:27:54.706743Z","shell.execute_reply.started":"2022-01-14T11:27:54.694942Z","shell.execute_reply":"2022-01-14T11:27:54.70551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_1=model_1.fit(train_dataset,epochs=5,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T11:31:44.853818Z","iopub.execute_input":"2022-01-14T11:31:44.854648Z","iopub.status.idle":"2022-01-14T12:06:35.088623Z","shell.execute_reply.started":"2022-01-14T11:31:44.854593Z","shell.execute_reply":"2022-01-14T12:06:35.08767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:06:44.778668Z","iopub.execute_input":"2022-01-14T12:06:44.779711Z","iopub.status.idle":"2022-01-14T12:06:48.493046Z","shell.execute_reply.started":"2022-01-14T12:06:44.779647Z","shell.execute_reply":"2022-01-14T12:06:48.491998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\nmodel_1_prediction_probability=model_1.predict(val_dataset)\nmodel_1_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:07:08.427592Z","iopub.execute_input":"2022-01-14T12:07:08.427897Z","iopub.status.idle":"2022-01-14T12:07:11.466545Z","shell.execute_reply.started":"2022-01-14T12:07:08.427867Z","shell.execute_reply":"2022-01-14T12:07:11.465712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\nmodel_1_prediction=tf.argmax(model_1_prediction_probability,axis=1)\nmodel_1_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:09:19.902527Z","iopub.execute_input":"2022-01-14T12:09:19.902865Z","iopub.status.idle":"2022-01-14T12:09:19.914069Z","shell.execute_reply.started":"2022-01-14T12:09:19.90283Z","shell.execute_reply":"2022-01-14T12:09:19.913369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_results=calculate_results(y_true=val_labels_encoded,y_pred=model_1_prediction)\nmodel_1_results","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:11:49.989478Z","iopub.execute_input":"2022-01-14T12:11:49.990513Z","iopub.status.idle":"2022-01-14T12:11:50.016161Z","shell.execute_reply.started":"2022-01-14T12:11:49.990466Z","shell.execute_reply":"2022-01-14T12:11:50.015214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Tensorflow Hub Pre trained Embedding and Feature Extractor </h3>","metadata":{}},{"cell_type":"code","source":"\n!pip install huggingface","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:26:51.381372Z","iopub.execute_input":"2022-01-14T12:26:51.381752Z","iopub.status.idle":"2022-01-14T12:27:03.290326Z","shell.execute_reply.started":"2022-01-14T12:26:51.381713Z","shell.execute_reply":"2022-01-14T12:27:03.289023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 2: Feature Extractor with pretrained token Embeddings\n# This is done to leverage the power of Transfer Learning\nimport tensorflow_hub as hub\ntf_hub_embedding_layers=hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",trainable=False,name=\"universal_sentence_encoder\")","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:34:35.986027Z","iopub.execute_input":"2022-01-14T12:34:35.986475Z","iopub.status.idle":"2022-01-14T12:34:56.102558Z","shell.execute_reply.started":"2022-01-14T12:34:35.986435Z","shell.execute_reply":"2022-01-14T12:34:56.101479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\n# For the Tensorflow embedding we are using the input shape needs to be in the form of an empty list\ninputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layers(inputs)\nx=layers.Dense(128,activation=\"relu\")(pretrained_embedding)\nx=layers.Dense(128,activation=\"relu\")(x)\n# Softmax because we are doing multiclass \noutputs=layers.Dense(5,activation=\"softmax\")(x)\nmodel_2=tf.keras.Model(inputs,outputs,name=\"model_2_transfer_learning\")\n# Compiling the Model\nmodel_2.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:57:03.43561Z","iopub.execute_input":"2022-01-14T12:57:03.436007Z","iopub.status.idle":"2022-01-14T12:57:03.575541Z","shell.execute_reply.started":"2022-01-14T12:57:03.435966Z","shell.execute_reply":"2022-01-14T12:57:03.574468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the Model\nhistory_model_2=model_2.fit(train_dataset,epochs=9,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T12:57:10.695704Z","iopub.execute_input":"2022-01-14T12:57:10.696016Z","iopub.status.idle":"2022-01-14T13:08:33.714434Z","shell.execute_reply.started":"2022-01-14T12:57:10.695987Z","shell.execute_reply":"2022-01-14T13:08:33.713731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating on the Validation Set\nmodel_2.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:09:35.827866Z","iopub.execute_input":"2022-01-14T13:09:35.828217Z","iopub.status.idle":"2022-01-14T13:09:45.662713Z","shell.execute_reply.started":"2022-01-14T13:09:35.82818Z","shell.execute_reply":"2022-01-14T13:09:45.661538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\nmodel_2_prediction_probability=model_2.predict(val_dataset)\nmodel_2_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:09:48.767712Z","iopub.execute_input":"2022-01-14T13:09:48.771046Z","iopub.status.idle":"2022-01-14T13:09:59.38117Z","shell.execute_reply.started":"2022-01-14T13:09:48.770972Z","shell.execute_reply":"2022-01-14T13:09:59.38033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\nmodel_2_prediction=tf.argmax(model_2_prediction_probability,axis=1)\nmodel_2_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:10:12.271464Z","iopub.execute_input":"2022-01-14T13:10:12.272582Z","iopub.status.idle":"2022-01-14T13:10:12.28255Z","shell.execute_reply.started":"2022-01-14T13:10:12.272523Z","shell.execute_reply":"2022-01-14T13:10:12.281525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2_results=calculate_results(y_true=val_labels_encoded,y_pred=model_2_prediction)\nmodel_2_results","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:10:16.14778Z","iopub.execute_input":"2022-01-14T13:10:16.148103Z","iopub.status.idle":"2022-01-14T13:10:16.170341Z","shell.execute_reply.started":"2022-01-14T13:10:16.148068Z","shell.execute_reply":"2022-01-14T13:10:16.169408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}