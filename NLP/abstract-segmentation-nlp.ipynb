{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the Project that I've been working on while taking this [Course](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/learn/)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-19T03:43:03.360517Z","iopub.execute_input":"2022-01-19T03:43:03.361407Z","iopub.status.idle":"2022-01-19T03:43:03.439975Z","shell.execute_reply.started":"2022-01-19T03:43:03.361282Z","shell.execute_reply":"2022-01-19T03:43:03.439199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:26.947926Z","iopub.execute_input":"2022-01-18T17:14:26.94869Z","iopub.status.idle":"2022-01-18T17:14:26.957056Z","shell.execute_reply.started":"2022-01-18T17:14:26.948643Z","shell.execute_reply":"2022-01-18T17:14:26.955826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir='../input/200000-abstracts-for-seq-sentence-classification/20k_abstracts_numbers_with_@/'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:26.959229Z","iopub.execute_input":"2022-01-18T17:14:26.960731Z","iopub.status.idle":"2022-01-18T17:14:26.976297Z","shell.execute_reply.started":"2022-01-18T17:14:26.960589Z","shell.execute_reply":"2022-01-18T17:14:26.973908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames=[data_dir + filename for filename in os.listdir(data_dir)]\nfilenames","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:26.982529Z","iopub.execute_input":"2022-01-18T17:14:26.983906Z","iopub.status.idle":"2022-01-18T17:14:26.997324Z","shell.execute_reply.started":"2022-01-18T17:14:26.983861Z","shell.execute_reply":"2022-01-18T17:14:26.996139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Preprocessing the Data","metadata":{}},{"cell_type":"code","source":"# Creating a function to read the txt Files\n# This function returns all the lines in the txt file as a list\ndef get_lines(filename):\n    with open(filename,\"r\") as f:\n        return f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:27.001862Z","iopub.execute_input":"2022-01-18T17:14:27.008726Z","iopub.status.idle":"2022-01-18T17:14:27.014027Z","shell.execute_reply.started":"2022-01-18T17:14:27.008672Z","shell.execute_reply":"2022-01-18T17:14:27.01302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the Lines in the Training Set\ntrain_data_lines=get_lines(data_dir+\"train.txt\")\n# train_data_lines[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:27.015432Z","iopub.execute_input":"2022-01-18T17:14:27.016172Z","iopub.status.idle":"2022-01-18T17:14:27.197688Z","shell.execute_reply.started":"2022-01-18T17:14:27.016125Z","shell.execute_reply":"2022-01-18T17:14:27.196602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <h4>\n Pre-processing the Data <br>\n List of dictionaries <br>\n Each line needs to be converted into a dictionary <br>\n Dictionary items are ordered, changeable, and does not allow duplicates.<br>\n Objectives, Methods, Results, Conclusins will all be Values to the 'TARGET' KEY<br>\n And the Text corresponding to them will be Values to the Key 'TEXT'<br>\n So each abstract would have about roughly 10-12 dicts for each statement<br>","metadata":{}},{"cell_type":"code","source":"# Preprocessing Functions\n# Returns a list of dictionaries of abstract's lines\n# Dict Format --> {'TARGET':'Background/Results/Objetive/Concludion','Text':'The actual statement'}\ndef preprocess_data(filename):\n    input_lines=get_lines(filename)\n    #This will be used to separte the abstracts from  one another using String mets\n    abstract_lines=\"\"\n    # Empty list of abstracts\n    abstract_samples=[]\n    for line in input_lines:\n        # Check for a new abstract\n        if line.startswith(\"###\"):\n            abstract_id=line\n            # And since we are in a new abstract we will Reset the abstract_lines\n            abstract_lines=\"\"\n        # Check for a new line \\n escape seq\n        elif line.isspace():\n            # Split the Lines of the abstract and will return a list of one abstract\n            abstract_line_split=abstract_lines.splitlines()\n            # Now we have to iterate through this singular abstract\n            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n                #  Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object.\n                # Create a empty Dict per line\n                line_data={}\n                # Split on the tab \\t esc seq\n                target_text_split=abstract_line.split(\"\\t\")\n                # Get the Label of the sentence as the Label\n                line_data[\"target\"]=target_text_split[0]\n                # Get the Text of the Lien as the Text Key\n                line_data[\"text\"]=target_text_split[1].lower()\n                # Also adding the Line Nnumber as it will also aid the model\n                line_data[\"line_number\"]=abstract_line_number\n                # Number of Lines in that particular abstract\n                line_data[\"total_lines\"]=len(abstract_line_split)-1\n                # Now we have to append them to the absract_samples list\n                abstract_samples.append(line_data)\n        # So if both the cases are not there then the line is a labelled sentence\n        else:\n            abstract_lines+=line\n    return abstract_samples\n                \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:27.202377Z","iopub.execute_input":"2022-01-18T17:14:27.204716Z","iopub.status.idle":"2022-01-18T17:14:27.235659Z","shell.execute_reply.started":"2022-01-18T17:14:27.204668Z","shell.execute_reply":"2022-01-18T17:14:27.234591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Get the data and preprocess it\ntrain_samples=preprocess_data(data_dir+\"train.txt\")\nval_samples=preprocess_data(data_dir+\"dev.txt\")\ntest_samples=preprocess_data(data_dir+\"test.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:27.240737Z","iopub.execute_input":"2022-01-18T17:14:27.243328Z","iopub.status.idle":"2022-01-18T17:14:28.50148Z","shell.execute_reply.started":"2022-01-18T17:14:27.243281Z","shell.execute_reply":"2022-01-18T17:14:28.500094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_samples),len(val_samples),len(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:28.503871Z","iopub.execute_input":"2022-01-18T17:14:28.504585Z","iopub.status.idle":"2022-01-18T17:14:28.514669Z","shell.execute_reply.started":"2022-01-18T17:14:28.504531Z","shell.execute_reply":"2022-01-18T17:14:28.512929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the Data\ntrain_samples[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:28.519581Z","iopub.execute_input":"2022-01-18T17:14:28.519948Z","iopub.status.idle":"2022-01-18T17:14:28.536754Z","shell.execute_reply.started":"2022-01-18T17:14:28.519851Z","shell.execute_reply":"2022-01-18T17:14:28.535438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we have to turn this data into a df\ntrain_df=pd.DataFrame(train_samples)\ntest_df=pd.DataFrame(test_samples)\nval_df=pd.DataFrame(val_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:28.53801Z","iopub.execute_input":"2022-01-18T17:14:28.540991Z","iopub.status.idle":"2022-01-18T17:14:29.003693Z","shell.execute_reply.started":"2022-01-18T17:14:28.538222Z","shell.execute_reply":"2022-01-18T17:14:29.002684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(11)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.005593Z","iopub.execute_input":"2022-01-18T17:14:29.006058Z","iopub.status.idle":"2022-01-18T17:14:29.02199Z","shell.execute_reply.started":"2022-01-18T17:14:29.005997Z","shell.execute_reply":"2022-01-18T17:14:29.020408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Spread of Data\ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.023944Z","iopub.execute_input":"2022-01-18T17:14:29.024345Z","iopub.status.idle":"2022-01-18T17:14:29.075366Z","shell.execute_reply.started":"2022-01-18T17:14:29.024288Z","shell.execute_reply":"2022-01-18T17:14:29.074257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Length of Lines\ntrain_df.total_lines.plot.hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.076842Z","iopub.execute_input":"2022-01-18T17:14:29.07715Z","iopub.status.idle":"2022-01-18T17:14:29.385682Z","shell.execute_reply.started":"2022-01-18T17:14:29.07712Z","shell.execute_reply":"2022-01-18T17:14:29.384444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of just the text Columns\ntrain_sentences=train_df[\"text\"].tolist()\ntest_sentences=test_df[\"text\"].tolist()\nval_sentences=val_df[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.3879Z","iopub.execute_input":"2022-01-18T17:14:29.388366Z","iopub.status.idle":"2022-01-18T17:14:29.4146Z","shell.execute_reply.started":"2022-01-18T17:14:29.388319Z","shell.execute_reply":"2022-01-18T17:14:29.413221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_sentences),len(test_sentences),len(val_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.416597Z","iopub.execute_input":"2022-01-18T17:14:29.416953Z","iopub.status.idle":"2022-01-18T17:14:29.425079Z","shell.execute_reply.started":"2022-01-18T17:14:29.416907Z","shell.execute_reply":"2022-01-18T17:14:29.423825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning the target Labels into Numeric Data\n# We have 5 main labels -> Backgroun, Objective,Methods, Results, Conclusion\n# We'll encode them both 1HEC and Simple Nuemrical\nfrom sklearn.preprocessing import OneHotEncoder\n# Tensorflow is incompaible with sparse matrices\none_hot_encoder=OneHotEncoder(sparse=False)\n# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\ntrain_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\nval_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\ntest_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\ntrain_labels_one_hot,val_labels_one_hot,test_labels_one_hot","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.427175Z","iopub.execute_input":"2022-01-18T17:14:29.427559Z","iopub.status.idle":"2022-01-18T17:14:29.572436Z","shell.execute_reply.started":"2022-01-18T17:14:29.42748Z","shell.execute_reply":"2022-01-18T17:14:29.570951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding also for Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain_labels_encoded=le.fit_transform(train_df[\"target\"])\ntest_labels_encoded=le.fit_transform(test_df[\"target\"])\nval_labels_encoded=le.fit_transform(val_df[\"target\"])\ntrain_labels_encoded, test_labels_encoded,val_labels_encoded","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.574746Z","iopub.execute_input":"2022-01-18T17:14:29.575079Z","iopub.status.idle":"2022-01-18T17:14:29.672853Z","shell.execute_reply.started":"2022-01-18T17:14:29.575031Z","shell.execute_reply":"2022-01-18T17:14:29.670645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retieving classes \nnum_classes=len(le.classes_)\nclass_names=le.classes_\nnum_classes,class_names","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.674614Z","iopub.execute_input":"2022-01-18T17:14:29.675385Z","iopub.status.idle":"2022-01-18T17:14:29.684672Z","shell.execute_reply.started":"2022-01-18T17:14:29.675333Z","shell.execute_reply":"2022-01-18T17:14:29.683396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>  Model -> Naive Bayes Model </h3> <br>\n<h4> TF-IDF Multinomial Naive Bayes Classifier </h4>","metadata":{}},{"cell_type":"code","source":"# Turning them to tensors\n# Baseline Model\n# tfidf turns text into Numbers with the Formula\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n# Creating a Pipeline\n# A pipeline takes Multiple Tuples\nmodel_0=Pipeline([\n    (\"tf-idf\",TfidfVectorizer()),\n    (\"clf\",MultinomialNB())\n    \n])\nmodel_0.fit(train_sentences,train_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:29.686967Z","iopub.execute_input":"2022-01-18T17:14:29.687404Z","iopub.status.idle":"2022-01-18T17:14:35.300341Z","shell.execute_reply.started":"2022-01-18T17:14:29.687288Z","shell.execute_reply":"2022-01-18T17:14:35.2994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation and Prediction\n# In scikit learn it is score for eval\nmodel_0.score(val_sentences,val_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:35.30199Z","iopub.execute_input":"2022-01-18T17:14:35.302406Z","iopub.status.idle":"2022-01-18T17:14:36.370514Z","shell.execute_reply.started":"2022-01-18T17:14:35.302357Z","shell.execute_reply":"2022-01-18T17:14:36.369539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make Predictions for the Baseline Model\nbaseline_predictions=model_0.predict(val_sentences)\nbaseline_predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:36.37527Z","iopub.execute_input":"2022-01-18T17:14:36.378286Z","iopub.status.idle":"2022-01-18T17:14:37.388426Z","shell.execute_reply.started":"2022-01-18T17:14:36.378223Z","shell.execute_reply":"2022-01-18T17:14:37.387385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n# Models for Calculating different evaluation metrics\n# Returns a dict of different metrics\ndef calculate_results(y_true, y_pred):\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:37.39482Z","iopub.execute_input":"2022-01-18T17:14:37.395296Z","iopub.status.idle":"2022-01-18T17:14:37.402529Z","shell.execute_reply.started":"2022-01-18T17:14:37.395254Z","shell.execute_reply":"2022-01-18T17:14:37.401531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_results(val_labels_encoded,baseline_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:37.404083Z","iopub.execute_input":"2022-01-18T17:14:37.405016Z","iopub.status.idle":"2022-01-18T17:14:37.429573Z","shell.execute_reply.started":"2022-01-18T17:14:37.404969Z","shell.execute_reply":"2022-01-18T17:14:37.428707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Conv 1D Model </h3>","metadata":{}},{"cell_type":"code","source":"# Vectorize the text and then create Embeddings\nfrom tensorflow.keras import layers\n# How long is each sentence on average\nsent_lens=[len(sentence.split()) for sentence in train_sentences]\navg_sent_lens=np.mean(sent_lens)\navg_sent_lens\n# sent_lens\n#  So we will need Padding and Truncating as the input shapes must be maintained","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:37.431107Z","iopub.execute_input":"2022-01-18T17:14:37.431424Z","iopub.status.idle":"2022-01-18T17:14:37.772351Z","shell.execute_reply.started":"2022-01-18T17:14:37.431381Z","shell.execute_reply":"2022-01-18T17:14:37.77138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(sent_lens,bins=9)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:37.773967Z","iopub.execute_input":"2022-01-18T17:14:37.774584Z","iopub.status.idle":"2022-01-18T17:14:39.07293Z","shell.execute_reply.started":"2022-01-18T17:14:37.774538Z","shell.execute_reply":"2022-01-18T17:14:39.07199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentile of length of sentences\noutput_seq_len=int(np.percentile(sent_lens,95))\noutput_seq_len\n# So 95% sentences are in length of 55","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:39.076239Z","iopub.execute_input":"2022-01-18T17:14:39.0771Z","iopub.status.idle":"2022-01-18T17:14:39.118437Z","shell.execute_reply.started":"2022-01-18T17:14:39.077048Z","shell.execute_reply":"2022-01-18T17:14:39.117131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a text Vectorization Layer\n# Mapping our text from words to Numbers\n# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n# Vocabulary size in the Research Paper is 68000\nmax_tokens=68000\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:39.12061Z","iopub.execute_input":"2022-01-18T17:14:39.121149Z","iopub.status.idle":"2022-01-18T17:14:39.134344Z","shell.execute_reply.started":"2022-01-18T17:14:39.121102Z","shell.execute_reply":"2022-01-18T17:14:39.133399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapt the Text Vectorizer to the Training Data\n# We have to adapt it to only the training data so that val and test data are not seen\n# Later it can be fitted to the two latter\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:39.135992Z","iopub.execute_input":"2022-01-18T17:14:39.13643Z","iopub.status.idle":"2022-01-18T17:14:51.488555Z","shell.execute_reply.started":"2022-01-18T17:14:39.136385Z","shell.execute_reply":"2022-01-18T17:14:51.487553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding out how many words are there  in the training vocabulary and which are  most common\n# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\ntrain_vocab=text_vectorizer.get_vocabulary()\n# Size of Vocab\nprint(len(train_vocab))\n# 5 Most Common Words in the Vocab\nprint(train_vocab[:5])\n# Least common 5 words in the vocab\nprint(train_vocab[-5:])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:51.490525Z","iopub.execute_input":"2022-01-18T17:14:51.490844Z","iopub.status.idle":"2022-01-18T17:14:51.723286Z","shell.execute_reply.started":"2022-01-18T17:14:51.490797Z","shell.execute_reply":"2022-01-18T17:14:51.722195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the config of our Text Vectorizer\ntext_vectorizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:51.724972Z","iopub.execute_input":"2022-01-18T17:14:51.725255Z","iopub.status.idle":"2022-01-18T17:14:51.735271Z","shell.execute_reply.started":"2022-01-18T17:14:51.725223Z","shell.execute_reply":"2022-01-18T17:14:51.73203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an Embedding Layer\n# More output dims , more emmbedding, more parameters to train\n# Masking the 0 considering themm as padding\ntoken_embed=layers.Embedding(input_dim=len(train_vocab),output_dim=128,mask_zero=True,name=\"token_embedding\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:51.737371Z","iopub.execute_input":"2022-01-18T17:14:51.737819Z","iopub.status.idle":"2022-01-18T17:14:51.745689Z","shell.execute_reply.started":"2022-01-18T17:14:51.737774Z","shell.execute_reply":"2022-01-18T17:14:51.744106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a Fast Loadinng Dataset with tf data API\n# https://www.tensorflow.org/guide/data_performance\n# https://www.tensorflow.org/guide/data\n# Turn our data into Tensorflow datasets\ntrain_dataset=tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\nval_dataset=tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\ntest_dataset=tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\ntrain_dataset\n# <TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>\n# Which indicates one Text Sample in first tuple, next tuple is (0,0,0,0,1) -> 1hc ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:51.747544Z","iopub.execute_input":"2022-01-18T17:14:51.748019Z","iopub.status.idle":"2022-01-18T17:14:52.86547Z","shell.execute_reply.started":"2022-01-18T17:14:51.747839Z","shell.execute_reply":"2022-01-18T17:14:52.864415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre fetching the data and making them into batches\n# Pre fetching reduces the Preparation time of Data taken by CPU\n# Pref-fetching in a Multi-threaded way Reduces time and Increases the amount of data as all cores can be utilized to Prepare the Data\n# The GPU will do the Computation\ntrain_dataset=train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset=val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset=test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntrain_dataset\n# Run the Previous steps as well this otherwie the Shapes will not be fixed\n# PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.867108Z","iopub.execute_input":"2022-01-18T17:14:52.867554Z","iopub.status.idle":"2022-01-18T17:14:52.881974Z","shell.execute_reply.started":"2022-01-18T17:14:52.867483Z","shell.execute_reply":"2022-01-18T17:14:52.880609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\ninputs=layers.Input(shape=(1,),dtype=tf.string)\ntext_vectors=text_vectorizer(inputs)\ntoken_embedding=token_embed(text_vectors)\nx=layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embedding)\nx=layers.GlobalAveragePooling1D()(x)\noutputs=layers.Dense(num_classes,activation=\"softmax\")(x)\n# Indirect way of creating the Modelling the op ip\nmodel_1=tf.keras.Model(inputs,outputs)\n# Compiling the Model\nmodel_1.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.883736Z","iopub.execute_input":"2022-01-18T17:14:52.884146Z","iopub.status.idle":"2022-01-18T17:14:52.961227Z","shell.execute_reply.started":"2022-01-18T17:14:52.884097Z","shell.execute_reply":"2022-01-18T17:14:52.960268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.962649Z","iopub.execute_input":"2022-01-18T17:14:52.963065Z","iopub.status.idle":"2022-01-18T17:14:52.978142Z","shell.execute_reply.started":"2022-01-18T17:14:52.963017Z","shell.execute_reply":"2022-01-18T17:14:52.977234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_1=model_1.fit(train_dataset,epochs=5,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.979196Z","iopub.execute_input":"2022-01-18T17:14:52.979418Z","iopub.status.idle":"2022-01-18T17:14:52.986405Z","shell.execute_reply.started":"2022-01-18T17:14:52.979389Z","shell.execute_reply":"2022-01-18T17:14:52.98519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.988753Z","iopub.execute_input":"2022-01-18T17:14:52.989147Z","iopub.status.idle":"2022-01-18T17:14:52.995784Z","shell.execute_reply.started":"2022-01-18T17:14:52.989101Z","shell.execute_reply":"2022-01-18T17:14:52.994629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\nmodel_1_prediction_probability=model_1.predict(val_dataset)\nmodel_1_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:52.997473Z","iopub.execute_input":"2022-01-18T17:14:52.997914Z","iopub.status.idle":"2022-01-18T17:14:53.00634Z","shell.execute_reply.started":"2022-01-18T17:14:52.997866Z","shell.execute_reply":"2022-01-18T17:14:53.005041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\nmodel_1_prediction=tf.argmax(model_1_prediction_probability,axis=1)\nmodel_1_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:53.007703Z","iopub.execute_input":"2022-01-18T17:14:53.008153Z","iopub.status.idle":"2022-01-18T17:14:53.019538Z","shell.execute_reply.started":"2022-01-18T17:14:53.008106Z","shell.execute_reply":"2022-01-18T17:14:53.018368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_results=calculate_results(y_true=val_labels_encoded,y_pred=model_1_prediction)\nmodel_1_results","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:53.021454Z","iopub.execute_input":"2022-01-18T17:14:53.021958Z","iopub.status.idle":"2022-01-18T17:14:53.030715Z","shell.execute_reply.started":"2022-01-18T17:14:53.021907Z","shell.execute_reply":"2022-01-18T17:14:53.029546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Tensorflow Hub Pre trained Embedding and Feature Extractor </h3>","metadata":{}},{"cell_type":"code","source":"\n!pip install huggingface","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:14:53.0325Z","iopub.execute_input":"2022-01-18T17:14:53.032874Z","iopub.status.idle":"2022-01-18T17:15:03.002522Z","shell.execute_reply.started":"2022-01-18T17:14:53.032804Z","shell.execute_reply":"2022-01-18T17:15:03.001383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 2: Feature Extractor with pretrained token Embeddings\n# This is done to leverage the power of Transfer Learning\nimport tensorflow_hub as hub\ntf_hub_embedding_layers=hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",trainable=False,name=\"universal_sentence_encoder\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:03.006613Z","iopub.execute_input":"2022-01-18T17:15:03.006899Z","iopub.status.idle":"2022-01-18T17:15:08.564631Z","shell.execute_reply.started":"2022-01-18T17:15:03.006868Z","shell.execute_reply":"2022-01-18T17:15:08.563544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\n# For the Tensorflow embedding we are using the input shape needs to be in the form of an empty list\ninputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layers(inputs)\nx=layers.Dense(128,activation=\"relu\")(pretrained_embedding)\nx=layers.Dense(128,activation=\"relu\")(x)\n# Softmax because we are doing multiclass \noutputs=layers.Dense(5,activation=\"softmax\")(x)\nmodel_2=tf.keras.Model(inputs,outputs,name=\"model_2_transfer_learning\")\n# Compiling the Model\nmodel_2.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.566237Z","iopub.execute_input":"2022-01-18T17:15:08.566568Z","iopub.status.idle":"2022-01-18T17:15:08.826811Z","shell.execute_reply.started":"2022-01-18T17:15:08.566509Z","shell.execute_reply":"2022-01-18T17:15:08.825832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the Model\nhistory_model_2=model_2.fit(train_dataset,epochs=9,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.830255Z","iopub.execute_input":"2022-01-18T17:15:08.830539Z","iopub.status.idle":"2022-01-18T17:15:08.835474Z","shell.execute_reply.started":"2022-01-18T17:15:08.830473Z","shell.execute_reply":"2022-01-18T17:15:08.834344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating on the Validation Set\nmodel_2.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.836894Z","iopub.execute_input":"2022-01-18T17:15:08.837682Z","iopub.status.idle":"2022-01-18T17:15:08.846786Z","shell.execute_reply.started":"2022-01-18T17:15:08.837634Z","shell.execute_reply":"2022-01-18T17:15:08.845726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\nmodel_2_prediction_probability=model_2.predict(val_dataset)\nmodel_2_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.848323Z","iopub.execute_input":"2022-01-18T17:15:08.849379Z","iopub.status.idle":"2022-01-18T17:15:08.858302Z","shell.execute_reply.started":"2022-01-18T17:15:08.849329Z","shell.execute_reply":"2022-01-18T17:15:08.857153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\nmodel_2_prediction=tf.argmax(model_2_prediction_probability,axis=1)\nmodel_2_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.860206Z","iopub.execute_input":"2022-01-18T17:15:08.860761Z","iopub.status.idle":"2022-01-18T17:15:08.872832Z","shell.execute_reply.started":"2022-01-18T17:15:08.860717Z","shell.execute_reply":"2022-01-18T17:15:08.87183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2_results=calculate_results(y_true=val_labels_encoded,y_pred=model_2_prediction)\nmodel_2_results","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.873951Z","iopub.execute_input":"2022-01-18T17:15:08.875581Z","iopub.status.idle":"2022-01-18T17:15:08.884263Z","shell.execute_reply.started":"2022-01-18T17:15:08.875536Z","shell.execute_reply":"2022-01-18T17:15:08.883209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Building a Model with Character level Embeddings </h3>","metadata":{}},{"cell_type":"code","source":"# So we first need to build a Character Layer Tokenizer \ndef split_chars(text):\n    return \" \".join(list(text))\n# Text Splitting \ntrain_chars=[split_chars(sentence) for sentence in train_sentences]\ntrain_chars[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:08.886041Z","iopub.execute_input":"2022-01-18T17:15:08.886435Z","iopub.status.idle":"2022-01-18T17:15:10.172464Z","shell.execute_reply.started":"2022-01-18T17:15:08.886389Z","shell.execute_reply":"2022-01-18T17:15:10.171512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_chars=[split_chars(sentence) for sentence in val_sentences]\ntest_chars=[split_chars(sentence) for sentence in test_sentences]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:10.174177Z","iopub.execute_input":"2022-01-18T17:15:10.174555Z","iopub.status.idle":"2022-01-18T17:15:10.53486Z","shell.execute_reply.started":"2022-01-18T17:15:10.174507Z","shell.execute_reply":"2022-01-18T17:15:10.533871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the Average Character Length\nchar_lens=[len(sentence) for sentence in train_sentences]\nmean_char_len=np.mean(char_lens)\nmean_char_len","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:10.536648Z","iopub.execute_input":"2022-01-18T17:15:10.536967Z","iopub.status.idle":"2022-01-18T17:15:10.619193Z","shell.execute_reply.started":"2022-01-18T17:15:10.536922Z","shell.execute_reply":"2022-01-18T17:15:10.618279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The meann is not the best way to gauge so we need a distribution and should take 95% CI\nplt.hist(char_lens,bins=6)\n\n# Finding the 95% CI Length\nci_char_len=int(np.percentile(char_lens,95))\nci_char_len\n# So 300 Characters must work","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:10.621016Z","iopub.execute_input":"2022-01-18T17:15:10.621401Z","iopub.status.idle":"2022-01-18T17:15:11.991355Z","shell.execute_reply.started":"2022-01-18T17:15:10.621356Z","shell.execute_reply":"2022-01-18T17:15:11.990366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up a Text Vectorization Layer\n# Getting the vocab size\nimport string\nalphanumeric=string.ascii_lowercase+string.digits+string.punctuation\nalphanumeric","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:11.993228Z","iopub.execute_input":"2022-01-18T17:15:11.993843Z","iopub.status.idle":"2022-01-18T17:15:12.002876Z","shell.execute_reply.started":"2022-01-18T17:15:11.993795Z","shell.execute_reply":"2022-01-18T17:15:12.001574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# +2 for space and OOV UNK Token\nmax_char_vocab=len(alphanumeric)+2\nchar_vectorizer=TextVectorization(max_tokens=max_char_vocab,output_sequence_length=ci_char_len+10,name=\"char_vectorizer\")\n# Adapt it to the Training set of sequences\nchar_vectorizer.adapt(train_chars)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:12.004946Z","iopub.execute_input":"2022-01-18T17:15:12.005389Z","iopub.status.idle":"2022-01-18T17:15:25.633578Z","shell.execute_reply.started":"2022-01-18T17:15:12.005345Z","shell.execute_reply":"2022-01-18T17:15:25.632557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_vocab=char_vectorizer.get_vocabulary()\nprint(len(char_vocab))\nchar_vocab[:5],char_vocab[-5:]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:25.635458Z","iopub.execute_input":"2022-01-18T17:15:25.635767Z","iopub.status.idle":"2022-01-18T17:15:25.64892Z","shell.execute_reply.started":"2022-01-18T17:15:25.635717Z","shell.execute_reply":"2022-01-18T17:15:25.647811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an Embedding Layer\n# As per the paper we have to make a 25 dim long Feature vector for Output dims\n# Each character gets embedded into a size 25 Feature Vector\n# Dont Mask Model \nchar_embed=layers.Embedding(input_dim=len(char_vocab),output_dim=25,mask_zero=False,name=\"character_embedding\")\n ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:25.650546Z","iopub.execute_input":"2022-01-18T17:15:25.651797Z","iopub.status.idle":"2022-01-18T17:15:25.658941Z","shell.execute_reply.started":"2022-01-18T17:15:25.651752Z","shell.execute_reply":"2022-01-18T17:15:25.657572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating character level Fast Loading Datasets\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset=tf.data.Dataset.from_tensor_slices((val_chars,val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:25.660504Z","iopub.execute_input":"2022-01-18T17:15:25.661649Z","iopub.status.idle":"2022-01-18T17:15:26.98753Z","shell.execute_reply.started":"2022-01-18T17:15:25.661604Z","shell.execute_reply":"2022-01-18T17:15:26.986551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_char_dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:26.989319Z","iopub.execute_input":"2022-01-18T17:15:26.989638Z","iopub.status.idle":"2022-01-18T17:15:26.996631Z","shell.execute_reply.started":"2022-01-18T17:15:26.989593Z","shell.execute_reply":"2022-01-18T17:15:26.99565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a BI-LSTM Model with the Character Level Embeddings\n# Also experimenting with a Conv1D with Char level Embeddings\n# Make Conv1D on chars only\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nchar_vectors = char_vectorizer(inputs)\nchar_embeddings = char_embed(char_vectors)\nx = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel_3 = tf.keras.Model(inputs=inputs,\n                         outputs=outputs,\n                         name=\"model_3_conv1D_char_embedding\")\n# Compile model\nmodel_3.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:26.998394Z","iopub.execute_input":"2022-01-18T17:15:26.999044Z","iopub.status.idle":"2022-01-18T17:15:27.075915Z","shell.execute_reply.started":"2022-01-18T17:15:26.998983Z","shell.execute_reply":"2022-01-18T17:15:27.074902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.077913Z","iopub.execute_input":"2022-01-18T17:15:27.078265Z","iopub.status.idle":"2022-01-18T17:15:27.093448Z","shell.execute_reply.started":"2022-01-18T17:15:27.078213Z","shell.execute_reply":"2022-01-18T17:15:27.092167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3_history = model_3.fit(train_char_dataset,\n                               epochs=6,\n                               validation_data=val_char_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.10295Z","iopub.execute_input":"2022-01-18T17:15:27.103181Z","iopub.status.idle":"2022-01-18T17:15:27.107582Z","shell.execute_reply.started":"2022-01-18T17:15:27.103152Z","shell.execute_reply":"2022-01-18T17:15:27.106424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the paper they have further used this BI-LSTM and then concatenated and then again passed it through another BILSTM Model.","metadata":{}},{"cell_type":"code","source":"model_3.evaluate(val_char_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.109747Z","iopub.execute_input":"2022-01-18T17:15:27.110736Z","iopub.status.idle":"2022-01-18T17:15:27.118813Z","shell.execute_reply.started":"2022-01-18T17:15:27.110679Z","shell.execute_reply":"2022-01-18T17:15:27.117674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\nmodel_3_prediction_probability=model_3.predict(val_char_dataset)\nmodel_3_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.120749Z","iopub.execute_input":"2022-01-18T17:15:27.121263Z","iopub.status.idle":"2022-01-18T17:15:27.130052Z","shell.execute_reply.started":"2022-01-18T17:15:27.121219Z","shell.execute_reply":"2022-01-18T17:15:27.128893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\nmodel_3_prediction=tf.argmax(model_3_prediction_probability,axis=1)\nmodel_3_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.131319Z","iopub.execute_input":"2022-01-18T17:15:27.131597Z","iopub.status.idle":"2022-01-18T17:15:27.14584Z","shell.execute_reply.started":"2022-01-18T17:15:27.131563Z","shell.execute_reply":"2022-01-18T17:15:27.144783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3_results=calculate_results(y_true=val_labels_encoded,y_pred=model_3_prediction)\nmodel_3_results","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.148068Z","iopub.execute_input":"2022-01-18T17:15:27.148458Z","iopub.status.idle":"2022-01-18T17:15:27.157032Z","shell.execute_reply.started":"2022-01-18T17:15:27.148412Z","shell.execute_reply":"2022-01-18T17:15:27.156066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Building a Model with Token + Character Embedding Model </h3>\n<h4> Concatenating Models ","metadata":{}},{"cell_type":"markdown","source":"During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.","metadata":{}},{"cell_type":"code","source":"# Building a Tf Dataset to load it faster\n# Combine chars and tokens into a dataset\ntrain_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\ntrain_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n\n# Prefetch and batch train data\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n# Preparing the same for Validation set\nval_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:27.158231Z","iopub.execute_input":"2022-01-18T17:15:27.158479Z","iopub.status.idle":"2022-01-18T17:15:29.318274Z","shell.execute_reply.started":"2022-01-18T17:15:27.158446Z","shell.execute_reply":"2022-01-18T17:15:29.317231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Buidling a Multi Input Model that can accpet multiple data streams\n# So we will build two BI-LSTM Models one with char embeddings and token Embeddings\n# Also there is going to be a dropout introduced \n# One more difference is we will be using the functional API instead or Sequential API\n# What it does is that it will go through steps together and not sequentially\n# So first we will crate a Token Model\ntoken_inputs=layers.Input(shape=[],dtype=tf.string,name=\"token_input\")\ntoken_embeddings = tf_hub_embedding_layers(token_inputs)\ntoken_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(token_inputs,token_output)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:29.32006Z","iopub.execute_input":"2022-01-18T17:15:29.320376Z","iopub.status.idle":"2022-01-18T17:15:29.436531Z","shell.execute_reply.started":"2022-01-18T17:15:29.320331Z","shell.execute_reply":"2022-01-18T17:15:29.435608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then we will create a Character Model\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embed(char_vectors)\n# The one with the char embeddings is passed through a BI-LSTM Model acc. to the Paper\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) \nchar_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:29.437945Z","iopub.execute_input":"2022-01-18T17:15:29.438258Z","iopub.status.idle":"2022-01-18T17:15:30.02723Z","shell.execute_reply.started":"2022-01-18T17:15:29.438214Z","shell.execute_reply":"2022-01-18T17:15:30.026248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating both the Models as mentioned\ntoken_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,char_model.output])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.029045Z","iopub.execute_input":"2022-01-18T17:15:30.029376Z","iopub.status.idle":"2022-01-18T17:15:30.041189Z","shell.execute_reply.started":"2022-01-18T17:15:30.029333Z","shell.execute_reply":"2022-01-18T17:15:30.040031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Combined Dropout and a dense Layer and then some dropout\ncombined_dropout = layers.Dropout(0.5)(token_char_concat)\ncombined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) \nfinal_dropout = layers.Dropout(0.5)(combined_dense)\n# Final Output, Label Pediction Layers\noutput_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.044483Z","iopub.execute_input":"2022-01-18T17:15:30.045229Z","iopub.status.idle":"2022-01-18T17:15:30.075042Z","shell.execute_reply.started":"2022-01-18T17:15:30.045182Z","shell.execute_reply":"2022-01-18T17:15:30.074137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Construct model with char and token inputs\n# 2 Inputs Token Model Input and Char Model Input \nmodel_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],outputs=output_layer,name=\"model_4_token_and_char_embeddings\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.076671Z","iopub.execute_input":"2022-01-18T17:15:30.077454Z","iopub.status.idle":"2022-01-18T17:15:30.088248Z","shell.execute_reply.started":"2022-01-18T17:15:30.077404Z","shell.execute_reply":"2022-01-18T17:15:30.087259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_4.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.090317Z","iopub.execute_input":"2022-01-18T17:15:30.090686Z","iopub.status.idle":"2022-01-18T17:15:30.116909Z","shell.execute_reply.started":"2022-01-18T17:15:30.090637Z","shell.execute_reply":"2022-01-18T17:15:30.115976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot hybrid token and character model and check if it has replicated the Paper\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_4,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.119546Z","iopub.execute_input":"2022-01-18T17:15:30.120096Z","iopub.status.idle":"2022-01-18T17:15:30.41966Z","shell.execute_reply.started":"2022-01-18T17:15:30.120048Z","shell.execute_reply":"2022-01-18T17:15:30.418424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the Model\n# None is the batch size in the shapes\nmodel_4.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.SGD(),metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.421634Z","iopub.execute_input":"2022-01-18T17:15:30.422784Z","iopub.status.idle":"2022-01-18T17:15:30.439209Z","shell.execute_reply.started":"2022-01-18T17:15:30.422734Z","shell.execute_reply":"2022-01-18T17:15:30.438173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on tokens and chars\nmodel_4_history = model_4.fit(train_char_token_dataset,epochs=5,validation_data=val_char_token_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.440878Z","iopub.execute_input":"2022-01-18T17:15:30.441405Z","iopub.status.idle":"2022-01-18T17:15:30.450008Z","shell.execute_reply.started":"2022-01-18T17:15:30.441358Z","shell.execute_reply":"2022-01-18T17:15:30.44882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caryying out similar exercise for Model 4\n# model_4_prediction_probability=model_4.predict(val_char_dataset)\n# model_4_prediction_probability\n# model_4_prediction=tf.argmax(model_4_prediction_probability,axis=1)\n# model_4_prediction\n# model_4_results=calculate_results(y_true=val_labels_encoded,y_pred=model_4_prediction)\n# model_4_results","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.451859Z","iopub.execute_input":"2022-01-18T17:15:30.452202Z","iopub.status.idle":"2022-01-18T17:15:30.461358Z","shell.execute_reply.started":"2022-01-18T17:15:30.452158Z","shell.execute_reply":"2022-01-18T17:15:30.460063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Pretained Token Embeddings + Character Emebeddings + Positional Embeddings </h3>","metadata":{}},{"cell_type":"code","source":"# Feature Engineering\n# Here we will take non obvious features from the Data and encoding them Numerically to aid the model\n# The order of Objectives, Methods, Results conclusion sequence is a non obvious feature\n# So we need to inject them into the Model and this is a part of the Feature Engineering\n# Encoding the Line Numbers\n# Engineering features incorporated need to be avaialable at test time\n# Positional Embeddings\ntrain_df[\"line_number\"].value_counts()\n# Almost 10K Abstracts have 10 + Lines","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:15:30.463773Z","iopub.execute_input":"2022-01-18T17:15:30.464214Z","iopub.status.idle":"2022-01-18T17:15:30.479898Z","shell.execute_reply.started":"2022-01-18T17:15:30.464169Z","shell.execute_reply":"2022-01-18T17:15:30.478766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution\n# Using TF To create one hot encoded tensors of Line Numbers\n# They can be also used as it is the Lines they are but 1HC is better\ntrain_line_numbers_one_hot=tf.one_hot(train_df[\"line_number\"].to_numpy(),depth=15)\nval_line_numbers_one_hot=tf.one_hot(val_df[\"line_number\"].to_numpy(),depth=15)\ntest_line_numbers_one_hot=tf.one_hot(test_df[\"line_number\"].to_numpy(),depth=15)\ntrain_line_numbers_one_hot[:10],train_line_numbers_one_hot.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:16:41.795017Z","iopub.execute_input":"2022-01-18T17:16:41.795841Z","iopub.status.idle":"2022-01-18T17:16:41.815918Z","shell.execute_reply.started":"2022-01-18T17:16:41.795802Z","shell.execute_reply":"2022-01-18T17:16:41.814764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to this similarly for Total Lines\ntrain_df[\"total_lines\"].value_counts()\n# We can take the Cutoff to e 20","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:18:00.128273Z","iopub.execute_input":"2022-01-18T17:18:00.128595Z","iopub.status.idle":"2022-01-18T17:18:00.138788Z","shell.execute_reply.started":"2022-01-18T17:18:00.128566Z","shell.execute_reply":"2022-01-18T17:18:00.13742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_total_lines_one_hot=tf.one_hot(train_df[\"total_lines\"].to_numpy(),depth=20)\nval_total_lines_one_hot=tf.one_hot(val_df[\"total_lines\"].to_numpy(),depth=20)\ntest_total_lines_one_hot=tf.one_hot(test_df[\"total_lines\"].to_numpy(),depth=20)\ntrain_total_lines_one_hot,train_total_lines_one_hot.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:19:55.934878Z","iopub.execute_input":"2022-01-18T17:19:55.935142Z","iopub.status.idle":"2022-01-18T17:19:55.960305Z","shell.execute_reply.started":"2022-01-18T17:19:55.935113Z","shell.execute_reply":"2022-01-18T17:19:55.958139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Buidling the Model\n# Token Level Model + Character Level Model + Positional Models (2) -> Line Numbers and Total Lines\n# Then we will Concatenate Model 1 and Model 2 \n# Then Combine 3,4 and Combination of 1&2 \n# Create an Output Layer\n# # # Model 1 # # #\ntoken_inputs=layers.Input(shape=[],dtype=tf.string,name=\"token_inputs\")\ntoken_embeddings=tf_hub_embedding_layers(token_inputs)\ntoken_outputs=layers.Dense(128,activation=\"relu\")(token_embeddings)\ntoken_model=tf.keras.Model(token_inputs,token_outputs)\n# Model 2 \nchar_inputs=layers.Input(shape=(1,),dtype=tf.string,name=\"char_inputs\")\nchar_vectors=char_vectorizer(char_inputs)\nchar_embeddings=char_embed(char_vectors)\n# 24 cos 25-26 is the vocab size and we need multiples of 8\nchar_bilstm=layers.Bidirectional(layers.LSTM(24))(char_embeddings)\nchar_model=tf.keras.Model(char_inputs,char_bilstm)\n\n#  Model 3 and Model 4\n# shape =(15,) as we have taken depth or size of line number till 15\nline_number_inputs=layers.Input(shape=(15,),dtype=tf.float32,name=\"line_number_inputs\")\nx=layers.Dense(32,activation=\"relu\")(line_number_inputs)\nline_number_model=tf.keras.Model(line_number_inputs,x)\n\ntotal_lines_inputs=layers.Input(shape=(20,),dtype=tf.float32,name=\"total_lines_inputs\")\ny=layers.Dense(32,activation=\"relu\")(total_lines_inputs)\ntotal_lines_model=tf.keras.Model(total_lines_inputs,y)\n\n# Concatenating the 1st and 2nd Model\ncombined_embeddings=layers.Concatenate(name=\"Hybrid_Merge_12\")([token_model.output,char_model.output])\n# Applying the Dropouts\nz=layers.Dense(256,activation=\"relu\")(combined_embeddings)\nz=layers.Dropout(0.5)(z)\n\n# Combining all of the above \nfinal_embeddings=layers.Concatenate(name=\"char_token_positional_embeddings\")([line_number_model.output,total_lines_model.output,z])\n\n# Creating the Output Layer for accepting the above layers and giving Probabilities\n\noutput_layer=layers.Dense(5,activation=\"softmax\",name=\"output_layer\")(final_embeddings)\n# Creating the Model\nmodel_5=tf.keras.Model([line_number_model.input,total_lines_model.input,token_model.input,char_model.input],output_layer,name=\"model_5_tribrid_embedding_model\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:47:38.648745Z","iopub.execute_input":"2022-01-18T17:47:38.64924Z","iopub.status.idle":"2022-01-18T17:47:39.453778Z","shell.execute_reply.started":"2022-01-18T17:47:38.649203Z","shell.execute_reply":"2022-01-18T17:47:39.45279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comma pachi je hoy e batch size maate hoy shapes ma\nmodel_5.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:47:42.183599Z","iopub.execute_input":"2022-01-18T17:47:42.184562Z","iopub.status.idle":"2022-01-18T17:47:42.209739Z","shell.execute_reply.started":"2022-01-18T17:47:42.184501Z","shell.execute_reply":"2022-01-18T17:47:42.208864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot hybrid token and character model and check if it has replicated the Paper\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_5,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:48:12.52675Z","iopub.execute_input":"2022-01-18T17:48:12.527051Z","iopub.status.idle":"2022-01-18T17:48:12.8616Z","shell.execute_reply.started":"2022-01-18T17:48:12.52702Z","shell.execute_reply":"2022-01-18T17:48:12.860504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n               optimizer=tf.keras.optimizers.SGD(),\n               metrics=[\"accuracy\"])\n# Label Smoothing is a regularization technique that introduces noise for the labels.","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:55:20.001975Z","iopub.execute_input":"2022-01-18T17:55:20.002343Z","iopub.status.idle":"2022-01-18T17:55:20.022218Z","shell.execute_reply.started":"2022-01-18T17:55:20.002311Z","shell.execute_reply":"2022-01-18T17:55:20.021167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Fast Loading Datasets for Model 5 with tf.data API\n# ##### IMP IMP IMP IMP IMP IMP IMP IMP IMP IMP ###########\n# The orders of line number, total lines, tokens, chars has been and must be maintained throughout the Model Operations\ntrain_char_token_pos_data=tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,train_total_lines_one_hot,train_sentences,train_chars))\ntrain_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\ntrain_char_token_pos_dataset=tf.data.Dataset.zip((train_char_token_pos_data,train_char_token_pos_labels))\ntrain_char_token_pos_dataset=train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\nval_char_token_pos_data=tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,val_total_lines_one_hot,val_sentences,val_chars))\nval_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_pos_dataset=tf.data.Dataset.zip((val_char_token_pos_data,val_char_token_pos_labels))\nval_char_token_pos_dataset=val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntest_char_token_pos_data=tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,test_total_lines_one_hot,test_sentences,test_chars))\ntest_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\ntest_char_token_pos_dataset=tf.data.Dataset.zip((test_char_token_pos_data,test_char_token_pos_labels))\ntest_char_token_pos_dataset=test_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:08:19.913395Z","iopub.execute_input":"2022-01-18T18:08:19.913744Z","iopub.status.idle":"2022-01-18T18:08:22.885257Z","shell.execute_reply.started":"2022-01-18T18:08:19.913713Z","shell.execute_reply":"2022-01-18T18:08:22.884176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_char_token_pos_dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:09:03.613907Z","iopub.execute_input":"2022-01-18T18:09:03.614175Z","iopub.status.idle":"2022-01-18T18:09:03.621238Z","shell.execute_reply.started":"2022-01-18T18:09:03.614146Z","shell.execute_reply":"2022-01-18T18:09:03.620197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_model_5 = model_5.fit(train_char_token_pos_dataset,\n                              epochs=9,\n                              validation_data=val_char_token_pos_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:32:42.868609Z","iopub.execute_input":"2022-01-18T18:32:42.868913Z","iopub.status.idle":"2022-01-18T19:11:29.186469Z","shell.execute_reply.started":"2022-01-18T18:32:42.868882Z","shell.execute_reply":"2022-01-18T19:11:29.185555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caryying out similar exercise for Model 4\nmodel_5_prediction_probability=model_5.predict(val_char_dataset)\nmodel_5_prediction_probability\nmodel_5_prediction=tf.argmax(model_5_prediction_probability,axis=1)\nmodel_5_prediction\nmodel_5_results=calculate_results(y_true=val_labels_encoded,y_pred=model_5_prediction)\nmodel_5_results","metadata":{},"execution_count":null,"outputs":[]}]}