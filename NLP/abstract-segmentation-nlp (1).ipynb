{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the Project that I've been working on while taking this [Course](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/learn/)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-17T07:22:06.171782Z","iopub.execute_input":"2022-01-17T07:22:06.172403Z","iopub.status.idle":"2022-01-17T07:22:06.187195Z","shell.execute_reply.started":"2022-01-17T07:22:06.172364Z","shell.execute_reply":"2022-01-17T07:22:06.186534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.188741Z","iopub.execute_input":"2022-01-17T07:22:06.189163Z","iopub.status.idle":"2022-01-17T07:22:06.198628Z","shell.execute_reply.started":"2022-01-17T07:22:06.18913Z","shell.execute_reply":"2022-01-17T07:22:06.19796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir='../input/200000-abstracts-for-seq-sentence-classification/20k_abstracts_numbers_with_@/'\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.200803Z","iopub.execute_input":"2022-01-17T07:22:06.201726Z","iopub.status.idle":"2022-01-17T07:22:06.212143Z","shell.execute_reply.started":"2022-01-17T07:22:06.201677Z","shell.execute_reply":"2022-01-17T07:22:06.21148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames=[data_dir + filename for filename in os.listdir(data_dir)]\nfilenames","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.213953Z","iopub.execute_input":"2022-01-17T07:22:06.214231Z","iopub.status.idle":"2022-01-17T07:22:06.228291Z","shell.execute_reply.started":"2022-01-17T07:22:06.214195Z","shell.execute_reply":"2022-01-17T07:22:06.227421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Preprocessing the Data","metadata":{}},{"cell_type":"code","source":"# Creating a function to read the txt Files\n# This function returns all the lines in the txt file as a list\ndef get_lines(filename):\n    with open(filename,\"r\") as f:\n        return f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.229852Z","iopub.execute_input":"2022-01-17T07:22:06.230668Z","iopub.status.idle":"2022-01-17T07:22:06.237239Z","shell.execute_reply.started":"2022-01-17T07:22:06.230618Z","shell.execute_reply":"2022-01-17T07:22:06.236297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the Lines in the Training Set\ntrain_data_lines=get_lines(data_dir+\"train.txt\")\n# train_data_lines[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.238754Z","iopub.execute_input":"2022-01-17T07:22:06.238968Z","iopub.status.idle":"2022-01-17T07:22:06.33788Z","shell.execute_reply.started":"2022-01-17T07:22:06.238942Z","shell.execute_reply":"2022-01-17T07:22:06.337211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <h4>\n Pre-processing the Data <br>\n List of dictionaries <br>\n Each line needs to be converted into a dictionary <br>\n Dictionary items are ordered, changeable, and does not allow duplicates.<br>\n Objectives, Methods, Results, Conclusins will all be Values to the 'TARGET' KEY<br>\n And the Text corresponding to them will be Values to the Key 'TEXT'<br>\n So each abstract would have about roughly 10-12 dicts for each statement<br>","metadata":{}},{"cell_type":"code","source":"# Preprocessing Functions\n# Returns a list of dictionaries of abstract's lines\n# Dict Format --> {'TARGET':'Background/Results/Objetive/Concludion','Text':'The actual statement'}\ndef preprocess_data(filename):\n    input_lines=get_lines(filename)\n    #This will be used to separte the abstracts from  one another using String mets\n    abstract_lines=\"\"\n    # Empty list of abstracts\n    abstract_samples=[]\n    for line in input_lines:\n        # Check for a new abstract\n        if line.startswith(\"###\"):\n            abstract_id=line\n            # And since we are in a new abstract we will Reset the abstract_lines\n            abstract_lines=\"\"\n        # Check for a new line \\n escape seq\n        elif line.isspace():\n            # Split the Lines of the abstract and will return a list of one abstract\n            abstract_line_split=abstract_lines.splitlines()\n            # Now we have to iterate through this singular abstract\n            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n                #  Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object.\n                # Create a empty Dict per line\n                line_data={}\n                # Split on the tab \\t esc seq\n                target_text_split=abstract_line.split(\"\\t\")\n                # Get the Label of the sentence as the Label\n                line_data[\"target\"]=target_text_split[0]\n                # Get the Text of the Lien as the Text Key\n                line_data[\"text\"]=target_text_split[1].lower()\n                # Also adding the Line Nnumber as it will also aid the model\n                line_data[\"line_number\"]=abstract_line_number\n                # Number of Lines in that particular abstract\n                line_data[\"total_lines\"]=len(abstract_line_split)-1\n                # Now we have to append them to the absract_samples list\n                abstract_samples.append(line_data)\n        # So if both the cases are not there then the line is a labelled sentence\n        else:\n            abstract_lines+=line\n    return abstract_samples\n                \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.338935Z","iopub.execute_input":"2022-01-17T07:22:06.339607Z","iopub.status.idle":"2022-01-17T07:22:06.3483Z","shell.execute_reply.started":"2022-01-17T07:22:06.33957Z","shell.execute_reply":"2022-01-17T07:22:06.347699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Get the data and preprocess it\ntrain_samples=preprocess_data(data_dir+\"train.txt\")\nval_samples=preprocess_data(data_dir+\"dev.txt\")\ntest_samples=preprocess_data(data_dir+\"test.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:06.3495Z","iopub.execute_input":"2022-01-17T07:22:06.34988Z","iopub.status.idle":"2022-01-17T07:22:07.093192Z","shell.execute_reply.started":"2022-01-17T07:22:06.349844Z","shell.execute_reply":"2022-01-17T07:22:07.092383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_samples),len(val_samples),len(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.094404Z","iopub.execute_input":"2022-01-17T07:22:07.09464Z","iopub.status.idle":"2022-01-17T07:22:07.101457Z","shell.execute_reply.started":"2022-01-17T07:22:07.09461Z","shell.execute_reply":"2022-01-17T07:22:07.1006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the Data\ntrain_samples[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.104436Z","iopub.execute_input":"2022-01-17T07:22:07.104651Z","iopub.status.idle":"2022-01-17T07:22:07.116994Z","shell.execute_reply.started":"2022-01-17T07:22:07.104624Z","shell.execute_reply":"2022-01-17T07:22:07.115973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we have to turn this data into a df\ntrain_df=pd.DataFrame(train_samples)\ntest_df=pd.DataFrame(test_samples)\nval_df=pd.DataFrame(val_samples)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.120072Z","iopub.execute_input":"2022-01-17T07:22:07.120527Z","iopub.status.idle":"2022-01-17T07:22:07.518161Z","shell.execute_reply.started":"2022-01-17T07:22:07.120488Z","shell.execute_reply":"2022-01-17T07:22:07.517273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(11)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.519318Z","iopub.execute_input":"2022-01-17T07:22:07.519555Z","iopub.status.idle":"2022-01-17T07:22:07.532604Z","shell.execute_reply.started":"2022-01-17T07:22:07.519525Z","shell.execute_reply":"2022-01-17T07:22:07.53148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Spread of Data\ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.534126Z","iopub.execute_input":"2022-01-17T07:22:07.534466Z","iopub.status.idle":"2022-01-17T07:22:07.580836Z","shell.execute_reply.started":"2022-01-17T07:22:07.534424Z","shell.execute_reply":"2022-01-17T07:22:07.579999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Length of Lines\ntrain_df.total_lines.plot.hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.582502Z","iopub.execute_input":"2022-01-17T07:22:07.583023Z","iopub.status.idle":"2022-01-17T07:22:07.830753Z","shell.execute_reply.started":"2022-01-17T07:22:07.582978Z","shell.execute_reply":"2022-01-17T07:22:07.829793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of just the text Columns\ntrain_sentences=train_df[\"text\"].tolist()\ntest_sentences=test_df[\"text\"].tolist()\nval_sentences=val_df[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.832335Z","iopub.execute_input":"2022-01-17T07:22:07.83268Z","iopub.status.idle":"2022-01-17T07:22:07.852812Z","shell.execute_reply.started":"2022-01-17T07:22:07.832636Z","shell.execute_reply":"2022-01-17T07:22:07.852077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_sentences),len(test_sentences),len(val_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.854179Z","iopub.execute_input":"2022-01-17T07:22:07.854601Z","iopub.status.idle":"2022-01-17T07:22:07.865212Z","shell.execute_reply.started":"2022-01-17T07:22:07.854566Z","shell.execute_reply":"2022-01-17T07:22:07.864385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning the target Labels into Numeric Data\n# We have 5 main labels -> Backgroun, Objective,Methods, Results, Conclusion\n# We'll encode them both 1HEC and Simple Nuemrical\nfrom sklearn.preprocessing import OneHotEncoder\n# Tensorflow is incompaible with sparse matrices\none_hot_encoder=OneHotEncoder(sparse=False)\n# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\ntrain_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\nval_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\ntest_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\ntrain_labels_one_hot,val_labels_one_hot,test_labels_one_hot","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:07.866605Z","iopub.execute_input":"2022-01-17T07:22:07.866843Z","iopub.status.idle":"2022-01-17T07:22:08.012841Z","shell.execute_reply.started":"2022-01-17T07:22:07.866814Z","shell.execute_reply":"2022-01-17T07:22:08.01195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding also for Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain_labels_encoded=le.fit_transform(train_df[\"target\"])\ntest_labels_encoded=le.fit_transform(test_df[\"target\"])\nval_labels_encoded=le.fit_transform(val_df[\"target\"])\ntrain_labels_encoded, test_labels_encoded,val_labels_encoded","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:08.014215Z","iopub.execute_input":"2022-01-17T07:22:08.014436Z","iopub.status.idle":"2022-01-17T07:22:08.111316Z","shell.execute_reply.started":"2022-01-17T07:22:08.014408Z","shell.execute_reply":"2022-01-17T07:22:08.110719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retieving classes \nnum_classes=len(le.classes_)\nclass_names=le.classes_\nnum_classes,class_names","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:08.112323Z","iopub.execute_input":"2022-01-17T07:22:08.112638Z","iopub.status.idle":"2022-01-17T07:22:08.117593Z","shell.execute_reply.started":"2022-01-17T07:22:08.11261Z","shell.execute_reply":"2022-01-17T07:22:08.117105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>  Model -> Naive Bayes Model </h3> <br>\n<h4> TF-IDF Multinomial Naive Bayes Classifier </h4>","metadata":{}},{"cell_type":"code","source":"# Turning them to tensors\n# Baseline Model\n# tfidf turns text into Numbers with the Formula\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n# Creating a Pipeline\n# A pipeline takes Multiple Tuples\nmodel_0=Pipeline([\n    (\"tf-idf\",TfidfVectorizer()),\n    (\"clf\",MultinomialNB())\n    \n])\nmodel_0.fit(train_sentences,train_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:08.118496Z","iopub.execute_input":"2022-01-17T07:22:08.118768Z","iopub.status.idle":"2022-01-17T07:22:13.656833Z","shell.execute_reply.started":"2022-01-17T07:22:08.11871Z","shell.execute_reply":"2022-01-17T07:22:13.656039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation and Prediction\n# In scikit learn it is score for eval\nmodel_0.score(val_sentences,val_labels_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:13.657907Z","iopub.execute_input":"2022-01-17T07:22:13.658293Z","iopub.status.idle":"2022-01-17T07:22:14.574586Z","shell.execute_reply.started":"2022-01-17T07:22:13.658257Z","shell.execute_reply":"2022-01-17T07:22:14.574018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make Predictions for the Baseline Model\nbaseline_predictions=model_0.predict(val_sentences)\nbaseline_predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:14.575538Z","iopub.execute_input":"2022-01-17T07:22:14.576231Z","iopub.status.idle":"2022-01-17T07:22:15.483626Z","shell.execute_reply.started":"2022-01-17T07:22:14.5762Z","shell.execute_reply":"2022-01-17T07:22:15.48271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n# Models for Calculating different evaluation metrics\n# Returns a dict of different metrics\ndef calculate_results(y_true, y_pred):\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:15.485024Z","iopub.execute_input":"2022-01-17T07:22:15.485261Z","iopub.status.idle":"2022-01-17T07:22:15.490906Z","shell.execute_reply.started":"2022-01-17T07:22:15.485231Z","shell.execute_reply":"2022-01-17T07:22:15.490296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_results(val_labels_encoded,baseline_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:15.491781Z","iopub.execute_input":"2022-01-17T07:22:15.492407Z","iopub.status.idle":"2022-01-17T07:22:15.518627Z","shell.execute_reply.started":"2022-01-17T07:22:15.492372Z","shell.execute_reply":"2022-01-17T07:22:15.51809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Conv 1D Model </h3>","metadata":{}},{"cell_type":"code","source":"# Vectorize the text and then create Embeddings\nfrom tensorflow.keras import layers\n# How long is each sentence on average\nsent_lens=[len(sentence.split()) for sentence in train_sentences]\navg_sent_lens=np.mean(sent_lens)\navg_sent_lens\n# sent_lens\n#  So we will need Padding and Truncating as the input shapes must be maintained","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:15.519983Z","iopub.execute_input":"2022-01-17T07:22:15.520426Z","iopub.status.idle":"2022-01-17T07:22:15.867718Z","shell.execute_reply.started":"2022-01-17T07:22:15.520393Z","shell.execute_reply":"2022-01-17T07:22:15.866974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(sent_lens,bins=9)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:15.869087Z","iopub.execute_input":"2022-01-17T07:22:15.869768Z","iopub.status.idle":"2022-01-17T07:22:17.118308Z","shell.execute_reply.started":"2022-01-17T07:22:15.869703Z","shell.execute_reply":"2022-01-17T07:22:17.117389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentile of length of sentences\noutput_seq_len=int(np.percentile(sent_lens,95))\noutput_seq_len\n# So 95% sentences are in length of 55","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:17.119549Z","iopub.execute_input":"2022-01-17T07:22:17.119885Z","iopub.status.idle":"2022-01-17T07:22:17.165153Z","shell.execute_reply.started":"2022-01-17T07:22:17.119848Z","shell.execute_reply":"2022-01-17T07:22:17.164195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a text Vectorization Layer\n# Mapping our text from words to Numbers\n# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n# Vocabulary size in the Research Paper is 68000\nmax_tokens=68000\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:17.170692Z","iopub.execute_input":"2022-01-17T07:22:17.170983Z","iopub.status.idle":"2022-01-17T07:22:17.182049Z","shell.execute_reply.started":"2022-01-17T07:22:17.170944Z","shell.execute_reply":"2022-01-17T07:22:17.18122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adapt the Text Vectorizer to the Training Data\n# We have to adapt it to only the training data so that val and test data are not seen\n# Later it can be fitted to the two latter\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:17.184065Z","iopub.execute_input":"2022-01-17T07:22:17.184305Z","iopub.status.idle":"2022-01-17T07:22:27.801641Z","shell.execute_reply.started":"2022-01-17T07:22:17.184277Z","shell.execute_reply":"2022-01-17T07:22:27.800789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding out how many words are there  in the training vocabulary and which are  most common\n# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\ntrain_vocab=text_vectorizer.get_vocabulary()\n# Size of Vocab\nprint(len(train_vocab))\n# 5 Most Common Words in the Vocab\nprint(train_vocab[:5])\n# Least common 5 words in the vocab\nprint(train_vocab[-5:])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:27.803145Z","iopub.execute_input":"2022-01-17T07:22:27.803448Z","iopub.status.idle":"2022-01-17T07:22:28.044654Z","shell.execute_reply.started":"2022-01-17T07:22:27.803407Z","shell.execute_reply":"2022-01-17T07:22:28.043722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the config of our Text Vectorizer\ntext_vectorizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:28.046292Z","iopub.execute_input":"2022-01-17T07:22:28.046653Z","iopub.status.idle":"2022-01-17T07:22:28.054931Z","shell.execute_reply.started":"2022-01-17T07:22:28.046609Z","shell.execute_reply":"2022-01-17T07:22:28.054092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an Embedding Layer\n# More output dims , more emmbedding, more parameters to train\n# Masking the 0 considering themm as padding\ntoken_embed=layers.Embedding(input_dim=len(train_vocab),output_dim=128,mask_zero=True,name=\"token_embedding\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:28.056369Z","iopub.execute_input":"2022-01-17T07:22:28.056625Z","iopub.status.idle":"2022-01-17T07:22:28.066722Z","shell.execute_reply.started":"2022-01-17T07:22:28.056594Z","shell.execute_reply":"2022-01-17T07:22:28.065883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a Fast Loadinng Dataset with tf data API\n# https://www.tensorflow.org/guide/data_performance\n# https://www.tensorflow.org/guide/data\n# Turn our data into Tensorflow datasets\ntrain_dataset=tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\nval_dataset=tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\ntest_dataset=tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\ntrain_dataset\n# <TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>\n# Which indicates one Text Sample in first tuple, next tuple is (0,0,0,0,1) -> 1hc ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:28.068093Z","iopub.execute_input":"2022-01-17T07:22:28.068341Z","iopub.status.idle":"2022-01-17T07:22:29.231034Z","shell.execute_reply.started":"2022-01-17T07:22:28.068305Z","shell.execute_reply":"2022-01-17T07:22:29.230352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre fetching the data and making them into batches\n# Pre fetching reduces the Preparation time of Data taken by CPU\n# Pref-fetching in a Multi-threaded way Reduces time and Increases the amount of data as all cores can be utilized to Prepare the Data\n# The GPU will do the Computation\ntrain_dataset=train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset=val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset=test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntrain_dataset\n# Run the Previous steps as well this otherwie the Shapes will not be fixed\n# PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.232151Z","iopub.execute_input":"2022-01-17T07:22:29.232358Z","iopub.status.idle":"2022-01-17T07:22:29.249244Z","shell.execute_reply.started":"2022-01-17T07:22:29.232332Z","shell.execute_reply":"2022-01-17T07:22:29.247994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\ninputs=layers.Input(shape=(1,),dtype=tf.string)\ntext_vectors=text_vectorizer(inputs)\ntoken_embedding=token_embed(text_vectors)\nx=layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embedding)\nx=layers.GlobalAveragePooling1D()(x)\noutputs=layers.Dense(num_classes,activation=\"softmax\")(x)\n# Indirect way of creating the Modelling the op ip\nmodel_1=tf.keras.Model(inputs,outputs)\n# Compiling the Model\nmodel_1.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.251943Z","iopub.execute_input":"2022-01-17T07:22:29.253034Z","iopub.status.idle":"2022-01-17T07:22:29.386522Z","shell.execute_reply.started":"2022-01-17T07:22:29.252924Z","shell.execute_reply":"2022-01-17T07:22:29.38546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.387887Z","iopub.execute_input":"2022-01-17T07:22:29.388216Z","iopub.status.idle":"2022-01-17T07:22:29.399674Z","shell.execute_reply.started":"2022-01-17T07:22:29.388183Z","shell.execute_reply":"2022-01-17T07:22:29.398539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_model_1=model_1.fit(train_dataset,epochs=5,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.40145Z","iopub.execute_input":"2022-01-17T07:22:29.402499Z","iopub.status.idle":"2022-01-17T07:22:29.407942Z","shell.execute_reply.started":"2022-01-17T07:22:29.402335Z","shell.execute_reply":"2022-01-17T07:22:29.406967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_1.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.40949Z","iopub.execute_input":"2022-01-17T07:22:29.410028Z","iopub.status.idle":"2022-01-17T07:22:29.420924Z","shell.execute_reply.started":"2022-01-17T07:22:29.40998Z","shell.execute_reply":"2022-01-17T07:22:29.419964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\n# model_1_prediction_probability=model_1.predict(val_dataset)\n# model_1_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.422391Z","iopub.execute_input":"2022-01-17T07:22:29.423287Z","iopub.status.idle":"2022-01-17T07:22:29.433795Z","shell.execute_reply.started":"2022-01-17T07:22:29.423236Z","shell.execute_reply":"2022-01-17T07:22:29.432808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\n# model_1_prediction=tf.argmax(model_1_prediction_probability,axis=1)\n# model_1_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.435752Z","iopub.execute_input":"2022-01-17T07:22:29.436353Z","iopub.status.idle":"2022-01-17T07:22:29.451242Z","shell.execute_reply.started":"2022-01-17T07:22:29.436302Z","shell.execute_reply":"2022-01-17T07:22:29.45051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_1_results=calculate_results(y_true=val_labels_encoded,y_pred=model_1_prediction)\n# model_1_results","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.452643Z","iopub.execute_input":"2022-01-17T07:22:29.453175Z","iopub.status.idle":"2022-01-17T07:22:29.463686Z","shell.execute_reply.started":"2022-01-17T07:22:29.453129Z","shell.execute_reply":"2022-01-17T07:22:29.463031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Tensorflow Hub Pre trained Embedding and Feature Extractor </h3>","metadata":{}},{"cell_type":"code","source":"\n!pip install huggingface","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:29.465001Z","iopub.execute_input":"2022-01-17T07:22:29.465662Z","iopub.status.idle":"2022-01-17T07:22:38.247532Z","shell.execute_reply.started":"2022-01-17T07:22:29.465586Z","shell.execute_reply":"2022-01-17T07:22:38.246545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model 2: Feature Extractor with pretrained token Embeddings\n# This is done to leverage the power of Transfer Learning\nimport tensorflow_hub as hub\ntf_hub_embedding_layers=hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",trainable=False,name=\"universal_sentence_encoder\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:38.249076Z","iopub.execute_input":"2022-01-17T07:22:38.249328Z","iopub.status.idle":"2022-01-17T07:22:43.470834Z","shell.execute_reply.started":"2022-01-17T07:22:38.249296Z","shell.execute_reply":"2022-01-17T07:22:43.470067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Model\n# For the Tensorflow embedding we are using the input shape needs to be in the form of an empty list\ninputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layers(inputs)\nx=layers.Dense(128,activation=\"relu\")(pretrained_embedding)\nx=layers.Dense(128,activation=\"relu\")(x)\n# Softmax because we are doing multiclass \noutputs=layers.Dense(5,activation=\"softmax\")(x)\nmodel_2=tf.keras.Model(inputs,outputs,name=\"model_2_transfer_learning\")\n# Compiling the Model\nmodel_2.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.472129Z","iopub.execute_input":"2022-01-17T07:22:43.472801Z","iopub.status.idle":"2022-01-17T07:22:43.677418Z","shell.execute_reply.started":"2022-01-17T07:22:43.472753Z","shell.execute_reply":"2022-01-17T07:22:43.676489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the Model\n# history_model_2=model_2.fit(train_dataset,epochs=9,validation_data=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.678867Z","iopub.execute_input":"2022-01-17T07:22:43.67926Z","iopub.status.idle":"2022-01-17T07:22:43.683701Z","shell.execute_reply.started":"2022-01-17T07:22:43.679214Z","shell.execute_reply":"2022-01-17T07:22:43.682859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating on the Validation Set\n# model_2.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.68547Z","iopub.execute_input":"2022-01-17T07:22:43.685843Z","iopub.status.idle":"2022-01-17T07:22:43.69654Z","shell.execute_reply.started":"2022-01-17T07:22:43.6858Z","shell.execute_reply":"2022-01-17T07:22:43.695709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\n# model_2_prediction_probability=model_2.predict(val_dataset)\n# model_2_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.697693Z","iopub.execute_input":"2022-01-17T07:22:43.698255Z","iopub.status.idle":"2022-01-17T07:22:43.707544Z","shell.execute_reply.started":"2022-01-17T07:22:43.698224Z","shell.execute_reply":"2022-01-17T07:22:43.706812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\n# model_2_prediction=tf.argmax(model_2_prediction_probability,axis=1)\n# model_2_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.708592Z","iopub.execute_input":"2022-01-17T07:22:43.709002Z","iopub.status.idle":"2022-01-17T07:22:43.721872Z","shell.execute_reply.started":"2022-01-17T07:22:43.708953Z","shell.execute_reply":"2022-01-17T07:22:43.721228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_2_results=calculate_results(y_true=val_labels_encoded,y_pred=model_2_prediction)\n# model_2_results","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.72295Z","iopub.execute_input":"2022-01-17T07:22:43.723559Z","iopub.status.idle":"2022-01-17T07:22:43.734053Z","shell.execute_reply.started":"2022-01-17T07:22:43.72351Z","shell.execute_reply":"2022-01-17T07:22:43.733227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Building a Model with Character level Embeddings </h3>","metadata":{}},{"cell_type":"code","source":"# So we first need to build a Character Layer Tokenizer \ndef split_chars(text):\n    return \" \".join(list(text))\n# Text Splitting \ntrain_chars=[split_chars(sentence) for sentence in train_sentences]\ntrain_chars[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:43.7353Z","iopub.execute_input":"2022-01-17T07:22:43.735782Z","iopub.status.idle":"2022-01-17T07:22:44.906244Z","shell.execute_reply.started":"2022-01-17T07:22:43.735722Z","shell.execute_reply":"2022-01-17T07:22:44.905596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_chars=[split_chars(sentence) for sentence in val_sentences]\ntest_chars=[split_chars(sentence) for sentence in test_sentences]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:44.907544Z","iopub.execute_input":"2022-01-17T07:22:44.908002Z","iopub.status.idle":"2022-01-17T07:22:45.301614Z","shell.execute_reply.started":"2022-01-17T07:22:44.907965Z","shell.execute_reply":"2022-01-17T07:22:45.300661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the Average Character Length\nchar_lens=[len(sentence) for sentence in train_sentences]\nmean_char_len=np.mean(char_lens)\nmean_char_len","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:45.303026Z","iopub.execute_input":"2022-01-17T07:22:45.303857Z","iopub.status.idle":"2022-01-17T07:22:45.382661Z","shell.execute_reply.started":"2022-01-17T07:22:45.303812Z","shell.execute_reply":"2022-01-17T07:22:45.382056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The meann is not the best way to gauge so we need a distribution and should take 95% CI\nplt.hist(char_lens,bins=6)\n\n# Finding the 95% CI Length\nci_char_len=int(np.percentile(char_lens,95))\nci_char_len\n# So 300 Characters must work","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:45.383615Z","iopub.execute_input":"2022-01-17T07:22:45.384093Z","iopub.status.idle":"2022-01-17T07:22:46.704907Z","shell.execute_reply.started":"2022-01-17T07:22:45.384063Z","shell.execute_reply":"2022-01-17T07:22:46.704036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up a Text Vectorization Layer\n# Getting the vocab size\nimport string\nalphanumeric=string.ascii_lowercase+string.digits+string.punctuation\nalphanumeric","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:46.706166Z","iopub.execute_input":"2022-01-17T07:22:46.706405Z","iopub.status.idle":"2022-01-17T07:22:46.71238Z","shell.execute_reply.started":"2022-01-17T07:22:46.706375Z","shell.execute_reply":"2022-01-17T07:22:46.711527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# +2 for space and OOV UNK Token\nmax_char_vocab=len(alphanumeric)+2\nchar_vectorizer=TextVectorization(max_tokens=max_char_vocab,output_sequence_length=ci_char_len+10,name=\"char_vectorizer\")\n# Adapt it to the Training set of sequences\nchar_vectorizer.adapt(train_chars)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:46.713442Z","iopub.execute_input":"2022-01-17T07:22:46.713662Z","iopub.status.idle":"2022-01-17T07:22:58.107954Z","shell.execute_reply.started":"2022-01-17T07:22:46.713636Z","shell.execute_reply":"2022-01-17T07:22:58.107278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_vocab=char_vectorizer.get_vocabulary()\nprint(len(char_vocab))\nchar_vocab[:5],char_vocab[-5:]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:58.111597Z","iopub.execute_input":"2022-01-17T07:22:58.112251Z","iopub.status.idle":"2022-01-17T07:22:58.12287Z","shell.execute_reply.started":"2022-01-17T07:22:58.112209Z","shell.execute_reply":"2022-01-17T07:22:58.121953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an Embedding Layer\n# As per the paper we have to make a 25 dim long Feature vector for Output dims\n# Each character gets embedded into a size 25 Feature Vector\n# Dont Mask Model \nchar_embed=layers.Embedding(input_dim=len(char_vocab),output_dim=25,mask_zero=False,name=\"character_embedding\")\n ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:58.124051Z","iopub.execute_input":"2022-01-17T07:22:58.124546Z","iopub.status.idle":"2022-01-17T07:22:58.137882Z","shell.execute_reply.started":"2022-01-17T07:22:58.124512Z","shell.execute_reply":"2022-01-17T07:22:58.137036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating character level Fast Loading Datasets\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset=tf.data.Dataset.from_tensor_slices((val_chars,val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:58.140131Z","iopub.execute_input":"2022-01-17T07:22:58.140378Z","iopub.status.idle":"2022-01-17T07:22:59.549168Z","shell.execute_reply.started":"2022-01-17T07:22:58.140349Z","shell.execute_reply":"2022-01-17T07:22:59.548555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_char_dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.550194Z","iopub.execute_input":"2022-01-17T07:22:59.550931Z","iopub.status.idle":"2022-01-17T07:22:59.557164Z","shell.execute_reply.started":"2022-01-17T07:22:59.550894Z","shell.execute_reply":"2022-01-17T07:22:59.5562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a BI-LSTM Model with the Character Level Embeddings\n# Also experimenting with a Conv1D with Char level Embeddings\n# Make Conv1D on chars only\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nchar_vectors = char_vectorizer(inputs)\nchar_embeddings = char_embed(char_vectors)\nx = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel_3 = tf.keras.Model(inputs=inputs,\n                         outputs=outputs,\n                         name=\"model_3_conv1D_char_embedding\")\n# Compile model\nmodel_3.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.558422Z","iopub.execute_input":"2022-01-17T07:22:59.558723Z","iopub.status.idle":"2022-01-17T07:22:59.62917Z","shell.execute_reply.started":"2022-01-17T07:22:59.558689Z","shell.execute_reply":"2022-01-17T07:22:59.62833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.630434Z","iopub.execute_input":"2022-01-17T07:22:59.630644Z","iopub.status.idle":"2022-01-17T07:22:59.638721Z","shell.execute_reply.started":"2022-01-17T07:22:59.630618Z","shell.execute_reply":"2022-01-17T07:22:59.637925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_3_history = model_3.fit(train_char_dataset,\n#                               epochs=6,\n#                               validation_data=val_char_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.639885Z","iopub.execute_input":"2022-01-17T07:22:59.640129Z","iopub.status.idle":"2022-01-17T07:22:59.649351Z","shell.execute_reply.started":"2022-01-17T07:22:59.640099Z","shell.execute_reply":"2022-01-17T07:22:59.648446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the paper they have further used this BI-LSTM and then concatenated and then again passed it through another BILSTM Model.","metadata":{}},{"cell_type":"code","source":"# model_3.evaluate(val_char_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.650494Z","iopub.execute_input":"2022-01-17T07:22:59.650722Z","iopub.status.idle":"2022-01-17T07:22:59.661143Z","shell.execute_reply.started":"2022-01-17T07:22:59.650696Z","shell.execute_reply":"2022-01-17T07:22:59.660175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Predictions In terms of Probabilities\n# model_3_prediction_probability=model_3.predict(val_char_dataset)\n# model_3_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.662343Z","iopub.execute_input":"2022-01-17T07:22:59.662668Z","iopub.status.idle":"2022-01-17T07:22:59.673251Z","shell.execute_reply.started":"2022-01-17T07:22:59.662622Z","shell.execute_reply":"2022-01-17T07:22:59.672436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now converting the Probabilities to classes\n# model_3_prediction=tf.argmax(model_3_prediction_probability,axis=1)\n# model_3_prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.674437Z","iopub.execute_input":"2022-01-17T07:22:59.674698Z","iopub.status.idle":"2022-01-17T07:22:59.688755Z","shell.execute_reply.started":"2022-01-17T07:22:59.674657Z","shell.execute_reply":"2022-01-17T07:22:59.687763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_3_results=calculate_results(y_true=val_labels_encoded,y_pred=model_3_prediction)\n# model_3_results","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.690323Z","iopub.execute_input":"2022-01-17T07:22:59.691211Z","iopub.status.idle":"2022-01-17T07:22:59.700354Z","shell.execute_reply.started":"2022-01-17T07:22:59.691159Z","shell.execute_reply":"2022-01-17T07:22:59.699666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Building a Model with Token + Character Embedding Model </h3>\n<h4> Concatenating Models ","metadata":{}},{"cell_type":"markdown","source":"During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.","metadata":{}},{"cell_type":"code","source":"# Building a Tf Dataset to load it faster\n# Combine chars and tokens into a dataset\ntrain_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\ntrain_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n\n# Prefetch and batch train data\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n# Preparing the same for Validation set\nval_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:22:59.70141Z","iopub.execute_input":"2022-01-17T07:22:59.702054Z","iopub.status.idle":"2022-01-17T07:23:01.907214Z","shell.execute_reply.started":"2022-01-17T07:22:59.702017Z","shell.execute_reply":"2022-01-17T07:23:01.906317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Buidling a Multi Input Model that can accpet multiple data streams\n# So we will build two BI-LSTM Models one with char embeddings and token Embeddings\n# Also there is going to be a dropout introduced \n# One more difference is we will be using the functional API instead or Sequential API\n# What it does is that it will go through steps together and not sequentially\n# So first we will crate a Token Model\ntoken_inputs=layers.Input(shape=[],dtype=tf.string,name=\"token_input\")\ntoken_embeddings = tf_hub_embedding_layers(token_inputs)\ntoken_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(token_inputs,token_output)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:01.908843Z","iopub.execute_input":"2022-01-17T07:23:01.909136Z","iopub.status.idle":"2022-01-17T07:23:01.998287Z","shell.execute_reply.started":"2022-01-17T07:23:01.909095Z","shell.execute_reply":"2022-01-17T07:23:01.997496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then we will create a Character Model\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embed(char_vectors)\n# The one with the char embeddings is passed through a BI-LSTM Model acc. to the Paper\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) \nchar_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.000015Z","iopub.execute_input":"2022-01-17T07:23:02.00033Z","iopub.status.idle":"2022-01-17T07:23:02.533659Z","shell.execute_reply.started":"2022-01-17T07:23:02.000287Z","shell.execute_reply":"2022-01-17T07:23:02.533091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenating both the Models as mentioned\ntoken_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,char_model.output])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.535298Z","iopub.execute_input":"2022-01-17T07:23:02.53589Z","iopub.status.idle":"2022-01-17T07:23:02.545754Z","shell.execute_reply.started":"2022-01-17T07:23:02.535843Z","shell.execute_reply":"2022-01-17T07:23:02.544865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Combined Dropout and a dense Layer and then some dropout\ncombined_dropout = layers.Dropout(0.5)(token_char_concat)\ncombined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) \nfinal_dropout = layers.Dropout(0.5)(combined_dense)\n# Final Output, Label Pediction Layers\noutput_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.547113Z","iopub.execute_input":"2022-01-17T07:23:02.547958Z","iopub.status.idle":"2022-01-17T07:23:02.575394Z","shell.execute_reply.started":"2022-01-17T07:23:02.5479Z","shell.execute_reply":"2022-01-17T07:23:02.57465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Construct model with char and token inputs\n# 2 Inputs Token Model Input and Char Model Input \nmodel_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],outputs=output_layer,name=\"model_4_token_and_char_embeddings\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.578833Z","iopub.execute_input":"2022-01-17T07:23:02.579054Z","iopub.status.idle":"2022-01-17T07:23:02.586335Z","shell.execute_reply.started":"2022-01-17T07:23:02.579027Z","shell.execute_reply":"2022-01-17T07:23:02.585806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_4.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.587284Z","iopub.execute_input":"2022-01-17T07:23:02.588036Z","iopub.status.idle":"2022-01-17T07:23:02.608641Z","shell.execute_reply.started":"2022-01-17T07:23:02.587986Z","shell.execute_reply":"2022-01-17T07:23:02.607825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot hybrid token and character model and check if it has replicated the Paper\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_4,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:02.60982Z","iopub.execute_input":"2022-01-17T07:23:02.610044Z","iopub.status.idle":"2022-01-17T07:23:03.75861Z","shell.execute_reply.started":"2022-01-17T07:23:02.610016Z","shell.execute_reply":"2022-01-17T07:23:03.757663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the Model\n# None is the batch size in the shapes\nmodel_4.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.SGD(),metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:03.760801Z","iopub.execute_input":"2022-01-17T07:23:03.761376Z","iopub.status.idle":"2022-01-17T07:23:03.776846Z","shell.execute_reply.started":"2022-01-17T07:23:03.761329Z","shell.execute_reply":"2022-01-17T07:23:03.776108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on tokens and chars\n# model_4_history = model_4.fit(train_char_token_dataset,epochs=5,validation_data=val_char_token_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T07:23:03.77879Z","iopub.execute_input":"2022-01-17T07:23:03.779647Z","iopub.status.idle":"2022-01-17T07:23:03.786709Z","shell.execute_reply.started":"2022-01-17T07:23:03.779601Z","shell.execute_reply":"2022-01-17T07:23:03.785979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caryying out similar exercise for Model 4\n# model_4_prediction_probability=model_4.predict(val_char_dataset)\n# model_4_prediction_probability\n# model_4_prediction=tf.argmax(model_4_prediction_probability,axis=1)\n# model_4_prediction\n# model_4_results=calculate_results(y_true=val_labels_encoded,y_pred=model_4_prediction)\n# model_4_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}